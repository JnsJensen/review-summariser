{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/dl21e22/.conda/envs/deep-learning/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 3090\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/tmp/ipykernel_1911084/3174333581.py:24: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
                  "  plt.style.use(\"seaborn\")\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "import os\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# use seaborn style for matplotlib\n",
            "plt.style.use(\"seaborn\")\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "# device = th.device(\"cpu\")\n",
            "\n",
            "fig_dir = \"img/\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' # Use simple GPT2 tokenizer for counting tokens\\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\nprint(f\"Original len: {len(df)}\")\\ndf_pruned = util.prune(df)\\nprint(f\"Pruned len: {len(df_pruned)}\")\\nutil.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\\n\\ndf_tokenized = util.tokenize(df_pruned, tokenizer)\\nprint(f\"Tokenized len: {len(df_tokenized)}\")\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df_tokenized[\\'reviewText\\'].arr.lengths()))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df_tokenized[\\'summary\\'].arr.lengths())))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\" # Use simple GPT2 tokenizer for counting tokens\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "print(f\"Original len: {len(df)}\")\n",
            "df_pruned = util.prune(df)\n",
            "print(f\"Pruned len: {len(df_pruned)}\")\n",
            "util.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "\n",
            "df_tokenized = util.tokenize(df_pruned, tokenizer)\n",
            "print(f\"Tokenized len: {len(df_tokenized)}\")\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df_tokenized['reviewText'].arr.lengths()))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df_tokenized['summary'].arr.lengths())))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, length = None, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        if length is not None:\n",
            "            # clip the dataset to length\n",
            "            length = min(length, len(self.df))\n",
            "            self.df = self.df.sample(length, shuffle=True)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=False, trim_offsets=True)\\nt.pad_token = t.eos_token\\nt.add_special_tokens({\"bos_token\": util.BOS_token})\\n\\n# Create the dataset\\ndataset = ReviewDataset(util.Paths.arts, t, device=device)\\n\\ndata_idx = 45\\n\\n# decode\\nprint(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\\nprint(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\\nprint(f\"Rating: {int(dataset[data_idx][2])}\")\\n\\n# max length is the max index of the vocabulary\\nMAX_LENGTH = len(t)\\nprint(f\"MAX_LENGTH: {MAX_LENGTH}\") '"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "\"\"\" t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=False, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.bidirectional = bidirectional\n",
            "        self.num_layers = num_layers\n",
            "        self.hidden_size = hidden_size\n",
            "        self.embedding = nn.Embedding(input_size, self.hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
            "        # self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=False)\n",
            "\n",
            "    # def forward(self, input, hidden):\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        context_vector, hidden = self.gru(embedded, hidden)\n",
            "        # context_vector, (hidden, cell_state) = self.lstm(embedded, (hidden, th.zeros_like(hidden, device=device)))\n",
            "        return context_vector, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        dimension_1 = self.num_layers * (2 if self.bidirectional else 1)\n",
            "        return th.zeros(dimension_1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, max_length, num_layers=1, bidirectional=True):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.bidirectional = bidirectional\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.max_length = max_length # The size of the vocabulary - len(tokenizer)\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
            "        # self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=False)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, context_vector):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), context_vector.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        # output, (hidden, cell_state) = self.lstm(output, (hidden, th.zeros_like(hidden, device=device)))\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        dimension_1 = self.num_layers * (2 if self.bidirectional else 1)\n",
            "        return th.zeros(dimension_1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Current run_dir: runs/073_mv_2.2.0_GRU_bi_nl_2_bs_64_lr_0.0005_tfr_0.5_hs_256_ds_8000\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Prepare for training\n",
            "debugging = True # For debugging prints\n",
            "model_version = \"2.2.0_GRU_bi\"\n",
            "n_epochs = 10\n",
            "batch_size = 64\n",
            "learning_rate = 0.0005\n",
            "teacher_forcing_ratio = 0.5\n",
            "hidden_size = 2**8 # 256\n",
            "dataset_size = 8000\n",
            "num_layers = 2 # LSTM or GRU layers\n",
            "bidirectional = False\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "run_dir = util.get_run_dir()\n",
            "# add run info to the directory\n",
            "run_dir += f\"_mv_{model_version}_nl_{num_layers}_bs_{batch_size}_lr_{learning_rate}_tfr_{teacher_forcing_ratio}_hs_{hidden_size}_ds_{dataset_size}\"\n",
            "print(f\"Current run_dir: {run_dir}\")\n",
            "# Readying the writer\n",
            "writer = SummaryWriter(run_dir)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # TODO: Check without the ignore_index\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "# Instantiate tokenizer\n",
            "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "tokenizer.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(tokenizer)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, num_layers=num_layers, max_length=MAX_LENGTH, bidirectional=bidirectional).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, tokenizer, length=dataset_size, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "\n",
            "# Get the data loaders\n",
            "train_loader, val_loader, test_loader = util.get_data_loaders(dataset, batch_size, split_ratios)\n",
            "val_iter = val_loader"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "def val(val_iter, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    with th.no_grad():\n",
            "        encoder.eval()\n",
            "        decoder.eval()\n",
            "        # We calcualte the loss and backpropagate every batch\n",
            "        # Reset the gradients\n",
            "        encoder_optimizer.zero_grad()\n",
            "        decoder_optimizer.zero_grad()\n",
            "\n",
            "        batch_loss = 0\n",
            "        words_in_batch = 0\n",
            "        correct_words_in_batch = 0\n",
            "\n",
            "        val_review_batch, val_summary_batch, val_rating_batch = next(val_iter)\n",
            "        for val_review, val_summary, val_rating in zip(val_review_batch, val_summary_batch, val_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Get the length of the review\n",
            "            review_length = val_review.shape[0]\n",
            "            summary_length = val_summary.shape[0]\n",
            "\n",
            "            # Create the encoder outputs tensor\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size * (2 if encoder.bidirectional else 1), device=device)\n",
            "\n",
            "            # Encoder forward pass\n",
            "            for ei in range(review_length):\n",
            "                encoder_output, encoder_hidden = encoder(val_review[ei], encoder_hidden)\n",
            "                encoder_outputs[ei] += encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            bos = th.tensor(tokenizer.bos_token_id).to(device)\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Initialize the decoder output\n",
            "            decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.long).fill_(tokenizer.pad_token_id)\n",
            "            decoder_output_sequence[0] = decoder_input\n",
            "\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Decoder forward pass\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(val_summary[1:]):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index+1] = decoder_input\n",
            "\n",
            "                # Count the correct words\n",
            "                if decoder_input.item() == target.item():\n",
            "                    correct_words_in_batch += 1\n",
            "\n",
            "                words_in_batch += 1\n",
            "                # Calculate the loss\n",
            "                batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                    #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                    break\n",
            "            \n",
            "            encoder.train()\n",
            "            decoder.train()\n",
            "\n",
            "            # Normalize the loss and accuracy\n",
            "            batch_loss /= words_in_batch\n",
            "            accuracy = correct_words_in_batch / words_in_batch\n",
            "\n",
            "            return accuracy, batch_loss.item(), decoder_output_sequence"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 3124, 373, 922, 290, 340, 318, 2705, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the color was good and it is soft\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 46928, 46928, 46928, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestinefascistfascistfascist\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[407, 262, 2033, 286, 5621, 1312, 1807, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " not the amount of sets i thought\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 32139, 46928, 46928, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine extractsfascistfascist\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[475, 612, 318, 407, 281, 2562, 835, 284, 3650, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " but there is not an easy way to store it\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[845, 3621, 3091, 329, 616, 22634, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " very nice box for my jewelry\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestineinged\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[14081, 7577, 867, 14093, 9040, 1541, 1216, 16548, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " lovely colors many brush tips already frayed\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[475, 991, 2499, 3734, 329, 616, 50142, 284, 779, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " but still works fine for my toddlers to use\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 32139, 24431, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine extractsingedingedingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 5543, 1842, 428, 9154, 3953, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i absolutely love this tape measure\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 41456, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine penetrating\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2722, 340, 287, 362, 1528, 290, 716, 20707, 351, 428, 4072, 793, 278, 9483, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " received it in 2 days and am delighted with this embossing folder\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 46928, 46928, 24431, 24431, 1756, 1756, 24431, 24431, 41456, 41456, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestinefascistfascistingedingeditionsitionsingedinged penetrating penetrating\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[3595, 40260, 286, 644, 1312, 373, 2045, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " poor imitation of what i was looking for\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 24431, 37918, 37918, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestineinged Sev Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[13779, 1486, 2420, 475, 355, 318, 340, 318, 407, 2861, 262, 1637, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " cute design text but as is it is not worth the money\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[18326, 290, 407, 257, 7209, 1720, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " inconsistent and not a smooth product\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[339, 318, 4441, 4950, 21641, 351, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " he is creating beautiful paintings with it\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 48712, 48712, 32139, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine896896 extractsinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[18364, 373, 2818, 1566, 340, 1625, 640, 284, 28602, 572, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " marker was perfect until it came time to erase off\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 46928, 32349, 48072, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestinefascist reuse quirksinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[11668, 5284, 319, 640, 290, 389, 2818, 329, 644, 1312, 761, 606, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " bags arrived on time and are perfect for what i need them for\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 32349, 32349, 24431, 24431, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestine reuse reuseingedingedingedingedinged\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 428, 481, 1239, 779, 10107, 7604, 10862, 757, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love this will never use manual hole punch again\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[257, 1276, 423, 329, 262, 35356, 6220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " a must have for the sewing kit\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[407, 644, 262, 10754, 2523, 379, 477, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " not what the posting shows at all\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 1842, 262, 2456, 319, 777, 28568, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i love the words on these stickers\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 3124, 318, 1049, 257, 3660, 460, 560, 2099, 7872, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the color is great a modern canary type yellow\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 46928, 46928, 46928, 41285, 24431, 41456, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestinefascistfascistfascist recollectioninged penetrating\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 561, 423, 8288, 340, 284, 307, 257, 1310, 5749, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i would have liked it to be a little bigger\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 41456, 48072, 41456, 48072, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine penetrating quirks penetrating quirksinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2243, 291, 18364, 11920, 416, 285, 560, 73, 1531, 264, 24929, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " copic marker carrier by maryjane sasser\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 24431, 24431, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestineingedingedingedingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[981, 340, 318, 3621, 326, 340, 318, 530, 279, 501, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " while it is nice that it is one pice\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 46928, 46928, 46928, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestinefascistfascistfascistingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[257, 64, 6979, 329, 616, 3367, 287, 1099, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " aa gift for my son in law\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 32349, 32349, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine reuse reuseingedinged\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[734, 812, 1468, 290, 407, 973, 1865, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " two years old and not used yet\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 318, 922, 3081, 772, 996, 1312, 6149, 262, 2642, 2546, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this is good quality even though i ordered the wrong size\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[340, 3073, 655, 588, 262, 4286, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " it looks just like the picture\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 1720, 379, 257, 6397, 2756, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great product at a reasonable price\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestine\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 29707, 670, 1049, 329, 644, 1312, 716, 18139, 606, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this pens work great for what i am needing them for\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 1339, 37609, 588, 36371, 18821, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the case smelled like rotten potatoes\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1643, 890, 475, 1312, 423, 616, 44571, 290, 1312, 1842, 606, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " bit long but i have my butterflies and i love them\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 48712, 48712, 26287, 35132, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine896896Average conferingedingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[340, 318, 922, 329, 257, 3155, 286, 1933, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " it is good for a couple of months\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 48712, 35931, 35931, 46928, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine896 Clarks Clarksfascistinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 428, 1517, 1049, 329, 17726, 41577, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love this thing great for portable knitting\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 32349, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine reuseingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[257, 1276, 423, 329, 534, 3348, 5977, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " a must have for your paper craft\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 32139, 32139, 46928, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine extracts extractsfascist\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[612, 318, 257, 1738, 484, 389, 7026, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " there is a reason they are cheap\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 35931, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestine Clarks\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2187, 1327, 3002, 318, 35737, 290, 7059, 276, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " whole hard cover is bowed and flexed\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1402, 599, 10141, 2147, 1969, 284, 9066, 2378, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " small spools nothing close to displayed item\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1266, 941, 82, 319, 716, 5168, 2832, 866, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " best studs on amazon hands down\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 41456, 41456, 41456, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine penetrating penetrating penetrating\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 3186, 428, 318, 257, 845, 1263, 900, 1720, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great products this is a very big set product\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 35931, 48072, 18151, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Clarks quirks smartphonesinged\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[466, 407, 2822, 290, 4296, 262, 6764, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " do not buy and update the description\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[257, 1276, 423, 329, 477, 4981, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " a must have for all models\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[16461, 284, 2603, 290, 27838, 2603, 290, 23275, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " sticks to mat and destroys mat and vinyl\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 21077, 17263, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine PalestineFound Palestine\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2495, 7577, 329, 34570, 502, 6220, 1402, 5301, 996, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " pretty colors for slime me kit small package though\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 24431, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestineingedingedingedinged\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 318, 416, 1290, 616, 4004, 4351, 9154, 284, 514, 329, 616, 5977, 4493, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this is by far my favorite transfer tape to us for my craft projects\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[845, 2705, 655, 826, 329, 11903, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " very soft just right for babies\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 777, 2208, 23408, 290, 257, 29175, 3348, 2587, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love these super sticky and a thicker paper material\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 6151, 428, 475, 340, 373, 845, 23408, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i loved this but it was very sticky\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 16636, 17263, 17263, 46928, 24431, 24431, 24431, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation founding Palestine Palestinefascistingedingedinged\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 6979, 329, 46458, 508, 1043, 257, 43836, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great gift for granddaughter who found a pearl\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 46928, 35931, 35931, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestinefascist Clarks Clarks\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 21181, 4704, 30712, 502, 290, 1312, 2921, 510, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the yarn thread whipped me and i gave up\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 561, 407, 4313, 7067, 777, 379, 477, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i would not recommend buying these at all\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 46928, 46928, 17951, 17951, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestinefascistfascistikediked\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[523, 1290, 1312, 1842, 606, 1111, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " so far i love them both\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[4950, 31133, 287, 257, 5445, 17379, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " beautiful glitter in a broken jar\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 46928, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestinefascist\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[7786, 2945, 284, 257, 15440, 5743, 407, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " sharpened to a precision edge not\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 716, 407, 262, 1266, 1048, 284, 2423, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i am not the best person to review\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[6275, 6491, 2139, 422, 262, 18583, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " excellent customer service from the seller\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[777, 19736, 766, 517, 286, 262, 1611, 326, 345, 651, 379, 262, 8872, 3650, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " these markers see more of the kind that you get at the dollar store\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2818, 329, 7325, 15508, 290, 15881, 2070, 364, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " perfect for creative teens and scrapbookers\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1807, 484, 561, 307, 475, 484, 991, 670, 655, 3734, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " thought they would be but they still work just fine\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 17263, 17263, 11264, 11264, 35931, 35931, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine Palestine Palestine Palestine Clark Clark Clarks Clarks\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1067, 323, 684, 644, 2073, 460, 1312, 910, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " crayons what else can i say\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[810, 423, 345, 587, 477, 616, 1204, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " where have you been all my life\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 481, 588, 284, 1441, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i will like to return it\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 1842, 37475, 290, 340, 3073, 1049, 319, 616, 42893, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i love frogs and it looks great on my bracelet\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[407, 1107, 262, 3280, 284, 616, 1917, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " not really the answer to my problem\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2818, 1720, 787, 1654, 345, 651, 262, 826, 2546, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " perfect product make sure you get the right size\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 373, 1257, 284, 1011, 319, 257, 5296, 10522, 1278, 4250, 1243, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this was fun to take on a trip abroad gluing things\n",
                  "Tokenized output:\n",
                  "[50257, 15602, 17263, 17263, 17263, 1756, 1756, 24431, 24431, 24431, 24431, 24431, 15924, 15924, 37918, 25588, 37918, 1776, 25588, 37918, 1776, 41456, 25588, 37918, 37918, 1776, 25588, 24431, 24431, 24431, 15924, 15924, 37918, 37918, 1776, 25588, 37918]\n",
                  "Detokenized output:\n",
                  " recommendation Palestine Palestine PalestineitionsitionsingedingedingedingedingedRoberRober Sev Flood Sev); Flood Sev); penetrating Flood Sev Sev); FloodingedingedingedRoberRober Sev Sev); Flood Sev\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "def train(learning_rate, val_loader, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "\n",
            "        val_iter = iter(val_loader)\n",
            "\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "            correct_words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                context_vector = th.zeros(MAX_LENGTH, encoder.hidden_size * (2 if encoder.bidirectional else 1), device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    context_vector[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(tokenizer.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.long).fill_(tokenizer.pad_token_id)\n",
            "                decoder_output_sequence[0] = decoder_input\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        #util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                        print(\"USING TEACHER FORCING\")\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary[1:]):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, context_vector)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index+1] = topi.squeeze().detach() # detach from history as input\n",
            "\n",
            "                        # Count the correct words\n",
            "                        if decoder_input.item() == target.item():\n",
            "                            correct_words_in_batch += 1\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                            #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                            break\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        #util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.CYAN])\n",
            "                        print(\"NOT USING TEACHER FORCING\")\n",
            "\n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary[1:]):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, context_vector)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index+1] = decoder_input\n",
            "\n",
            "                        # Count the correct words\n",
            "                        if decoder_input.item() == target.item():\n",
            "                            correct_words_in_batch += 1\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                            #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                            break\n",
            "                \n",
            "                if debugging:\n",
            "                    # print tokenized output\n",
            "                    #util.print_mod(\"Target tokenized:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Target tokenized:\")\n",
            "                    print(summary.tolist())\n",
            "                    # util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(\"Target Sequence:\")\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    #util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Tokenized output:\")\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    #util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Detokenized output:\")\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            \n",
            "            # Normalize the loss\n",
            "            batch_loss /= words_in_batch\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "            # Accuracy\n",
            "            accuracy = correct_words_in_batch / words_in_batch\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss and accuracy\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss, epoch * len(train_loader) + batch_idx)\n",
            "            writer.add_scalar(\"Accuracy/train\", accuracy, epoch * len(train_loader) + batch_idx)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}\")\n",
            "            # util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}\", [util.Modifiers.Colors.MAGENTA, util.Modifiers.Styles.BOLD])\n",
            "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}, Accuracy: {accuracy}\")\n",
            "\n",
            "            # Validate the model\n",
            "            if batch_idx % 5 == 0:\n",
            "                val_acc, val_loss, val_sequence = val(val_iter, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
            "                val_sequence_detokenized = dataset.detokenize(val_sequence)\n",
            "                writer.add_scalar(\"Loss/val\", val_loss, epoch * len(train_loader) + batch_idx)\n",
            "                writer.add_scalar(\"Accuracy/val\", val_acc, epoch * len(train_loader) + batch_idx)\n",
            "\n",
            "train(learning_rate, val_loader, n_epochs, train_loader, val_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "deep-learning",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "6e6b2526eed71276f347b2691ea1782fae1a1918ffb866c7f03c7485a367b288"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
