{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "# define device\n",
    "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
    "\n",
    "# print device properties according to device type\n",
    "if device.type == \"cuda\":\n",
    "    print(th.cuda.get_device_name(device))\n",
    "elif device.type == \"mps\":\n",
    "    print(th.backends.mps.get_device_name(device))\n",
    "elif device.type == \"cpu\":\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          reviewtext  overall  \\\n",
      "0  I had big expectations because I love English ...        2   \n",
      "1  I highly recommend this series. It is a must f...        5   \n",
      "2  This one is a real snoozer. Don't believe anyt...        1   \n",
      "3  Mysteries are interesting.  The tension betwee...        4   \n",
      "4  This show always is excellent, as far as briti...        5   \n",
      "\n",
      "                          summary  \n",
      "0      A little bit boring for me  \n",
      "1           Excellent Grown Up TV  \n",
      "2           Way too boring for me  \n",
      "3     Robson Green is mesmerizing  \n",
      "4  Robson green and great writing  \n",
      "\n",
      "Max length of review text:  18152\n",
      "Max length of summary:  151\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "data_dir = \"datasets/\"\n",
    "data_all = \"All_Amazon_Review_5.json\"\n",
    "data_video = \"Amazon_Instant_Video_5.json\"\n",
    "\n",
    "# read data with pandas\n",
    "df = pd.read_json(data_dir + data_video, lines=True)\n",
    "\n",
    "# lower case all headers\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# keep only the review text, rating, and summary\n",
    "df = df[['reviewtext', 'overall', 'summary']]\n",
    "print(df.head())\n",
    "\n",
    "# find max length of review text with numpy\n",
    "max_review_len = np.max(df['reviewtext'].apply(len))\n",
    "print(\"\\nMax length of review text: \", max_review_len)\n",
    "# find max length of summary with numpy\n",
    "max_summary_len = np.max(df['summary'].apply(len))\n",
    "print(\"Max length of summary: \", max_summary_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset from pandas dataframe\n",
    "# defines a voacbulary of words and converts the review text to a list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset from pandas dataframe\n",
    "# defines a voacbulary of words and converts the review text to a list of indices\n",
    "# beware of symbols like ., !, ? etc.\n",
    "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
    "\n",
    "class ReviewDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vocab = set()\n",
    "\n",
    "        # call the function to create the vocabulary\n",
    "        self.create_vocab()\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.vocab2idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.idx2vocab = {i: word for i, word in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def create_vocab(self):\n",
    "        # create the shared vocabulary\n",
    "        for review in self.df['reviewtext']:\n",
    "            for word in review.split():\n",
    "                self.vocab.add(word)\n",
    "        for summary in self.df['summary']:\n",
    "            for word in summary.split():\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.df.iloc[idx, 0].split()\n",
    "        review = [self.vocab2idx[word] for word in review]\n",
    "        review = th.tensor(review, dtype=th.long)\n",
    "        review = th.nn.functional.pad(review, (0, max_review_len - len(review)))\n",
    "        rating = self.df.iloc[idx, 1]\n",
    "        rating = th.tensor(rating, dtype=th.long)\n",
    "        summary = self.df.iloc[idx, 2].split()\n",
    "        summary = [self.vocab2idx[word] for word in summary]\n",
    "        summary = th.tensor(summary, dtype=th.long)\n",
    "        summary = th.nn.functional.pad(summary, (0, max_summary_len - len(summary)))\n",
    "\n",
    "        # move tensors to device\n",
    "        review = review.to(device)\n",
    "        rating = rating.to(device)\n",
    "        summary = summary.to(device)\n",
    "        \n",
    "        return review, rating, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 45253, 122627,  88635,  ...,      0,      0,      0], device='cuda:0'), tensor(2, device='cuda:0'), tensor([ 19675, 136056,  88914,  90146, 117201, 138804,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# test the dataset\n",
    "dataset = ReviewDataset(df)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model\n",
    "uses context aware word embedding\n",
    "multi-task network\n",
    "\n",
    "Input: takes in a review string\n",
    "Task 1: output a summary string of the input review with a max length defined by the dataset\n",
    "Task 2: output a rating of the input review as a float 0-1\n",
    "\n",
    "Use an encoder decoder setup with one decoder for each task\n",
    "\"\"\"\n",
    "class Summariser(th.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_review_len, max_summary_len):\n",
    "        super(Summariser, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_review_len = max_review_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "        self.embedding = th.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.decoder1 = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True)\n",
    "        self.decoder2 = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True)\n",
    "        self.linear1 = th.nn.Linear(embedding_dim, vocab_size)\n",
    "        self.linear2 = th.nn.Linear(embedding_dim, 1)\n",
    "        self.softmax = th.nn.Softmax(dim=2)\n",
    "        self.sigmoid = th.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.encoder(x)\n",
    "        x1, _ = self.decoder1(x)\n",
    "        x2, _ = self.decoder2(x)\n",
    "        x1 = self.linear1(x1)\n",
    "        x1 = self.softmax(x1)\n",
    "        x2 = self.linear2(x2)\n",
    "        x2 = self.sigmoid(x2)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset preparation\n",
    "Use the ReviewDataset to create a DataLoader\n",
    "Splitting the train, validation, and test sets\n",
    "\"\"\"\n",
    "# initialise the dataset\n",
    "dataset = ReviewDataset(df)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# shrink dataset for testing\n",
    "dataset_size = 500\n",
    "dataset = th.utils.data.Subset(dataset, range(dataset_size))\n",
    "\n",
    "# split the dataset\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# create the dataloaders\n",
    "batch_size = 32\n",
    "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = th.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18152]) torch.Size([32]) torch.Size([32, 151])\n"
     ]
    }
   ],
   "source": [
    "# test the dataloader\n",
    "train_loader_iter = iter(train_loader)\n",
    "x, y, z = next(train_loader_iter)\n",
    "print(x.shape, y.shape, z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.69 GiB (GPU 0; 8.00 GiB total capacity; 3.93 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [102], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m y_pred1, y_pred2 \u001b[39m=\u001b[39m model(review)\n\u001b[0;32m     28\u001b[0m \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss1 \u001b[39m=\u001b[39m loss_fn1(y_pred1, summary)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [86], line 30\u001b[0m, in \u001b[0;36mSummariser.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m---> 30\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     31\u001b[0m     x1, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder1(x)\n\u001b[0;32m     32\u001b[0m     x2, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    771\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.69 GiB (GPU 0; 8.00 GiB total capacity; 3.93 GiB already allocated; 0 bytes free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "# initialise the model\n",
    "# take into account if it is a subset of the dataset\n",
    "model = Summariser(dataset.dataset.vocab_size, 256, max_review_len, max_summary_len)\n",
    "model = model.to(device)\n",
    "\n",
    "# define the loss functions\n",
    "loss_fn1 = th.nn.CrossEntropyLoss()\n",
    "loss_fn2 = th.nn.BCELoss()\n",
    "\n",
    "# define the optimiser\n",
    "optimiser = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# train the model\n",
    "for epoch in range(epochs):\n",
    "    for review, rating, summary in train_loader:\n",
    "        # zero the gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        y_pred1, y_pred2 = model(review)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss1 = loss_fn1(y_pred1, summary)\n",
    "        loss2 = loss_fn2(y_pred2, rating.unsqueeze(1).float())\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimiser.step()\n",
    "\n",
    "    # print the loss\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69323fde9fc4d20886c37f6bdc4a05b4e3b82913212d2329f781a907e0bb44ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
