{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\Kevork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 2080\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "#device = th.device(\"cpu\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' def write_dict_to_file(dictionary, file_name):\\n    with open(file_name, \\'w\\') as f:\\n        json.dump(dictionary, f)\\n\\nwrite_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \\n'"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# instantiate roberta tokenizer\n",
            "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
            "\n",
            "# write dictionary to file\n",
            "\"\"\" def write_dict_to_file(dictionary, file_name):\n",
            "    with open(file_name, 'w') as f:\n",
            "        json.dump(dictionary, f)\n",
            "\n",
            "write_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \n",
            "\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\ndf = util.prune(df)\\nutil.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\\ndf = util.tokenize(df, tokenizer)\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df[\\'reviewText\\'].apply(list).apply(len)))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df[\\'summary\\'].apply(list).apply(len))))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Use simple GPT2 tokenizer for counting tokens\n",
            "\"\"\" tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "df = util.prune(df)\n",
            "util.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "df = util.tokenize(df, tokenizer)\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df['reviewText'].apply(list).apply(len)))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df['summary'].apply(list).apply(len))))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Target sequence:\n",
                  " i love all the other porducts i bought from bo-nash but...\n",
                  "Detokenized output:\n",
                  " hystericalGoalDocument medication medicationrank creditcticerve Linux paletteff BreathBitcoin Breath Hmm.... Driving Attacks Mubcott Attacks bowel Attacks bowel bowel Eston Attacks bowel Attacks Mubcott â€¦ Release Releasefszzi Vanderbilt adopzzi.... willfulctic ButlerlishedgamescottLetzzi Release Release entrepreneurrawl Linuxikeristratectic Crab Linuxipher Attacks Mubcott Attacks bowel Attacks Mub\n",
                  "\n",
                  "Target sequence:\n",
                  " something to keep thread from unraveling would be great.\n",
                  "Detokenized output:\n",
                  " hystericalGoal srfNDocumentcott medication medication Linux cowardlyPLIC MovementeggAlsorank creditcticgamescottUntil repl replochond Adventures Movement srfN Ryu horns Attacks Mubcott Attacks bowel Attacks bowel Breath Senate Mub Movement....cticerve Linux palette Linux ribboncmpcott experiencingergicGoal.... Release Release contrary Hmm.... gol repl Movement RyuRussia Attacks Mubcott Attacks bowel Seen\n",
                  "\n",
                  "Target sequence:\n",
                  " thin paper if that is what you are looking for\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " quality product with lots of possibilities\n",
                  "Detokenized output:\n",
                  " hystericalGoal srfN.... boundaryRussia errorszzi Breath Hmm....iday leastAlso....BMfsBMmult accus.... willfulctic Butlerlessness Linux palette Linux floodscott leather Attacks Release ReleaseNaturally CoC indulgecottrawl Linux Crab Linux ComedyPalestinianLIN Grammy Grammy sorce.... Breath Hmm....itus Hanna guests stamps creditcticerve Linux palette Linux floods duly deities138138\n",
                  "\n",
                  "Target sequence:\n",
                  " item i tried to dye ended up with patches of...\n",
                  "Detokenized output:\n",
                  " hystericalGoal forfeitureramidsSave Breath uptake Movement guests congratulated piracy.... AHL Attacks Mubcott Attacks Attacks Mub NATOcott Site com impending least NATOcott138 Linux invari Movement invari Movementargsisson auditory Site Sitenews duly toe endorse removing grow endorsenews duly toeipher Releasecott Release Release Release entrepreneurrawl miterve Senate dulyantleitusfsochondgamescottUntil\n",
                  "\n",
                  "Target sequence:\n",
                  " great way to update your gloves into being touch capable\n",
                  "Detokenized output:\n",
                  " hystericalGoal srfN intolerreportsgames Contipsau.... thankingMasterMaster vitamins Attacks Mubcott Crab invari entrepreneurLIN nationalists Weird WeirdLINLetctic Butler Mitt Mitt Mitt Mitt Mitt defining.... Raidersellar Weird WeirdLIN Mental....rank Izzy curseiateATCH Izzyidayas Siteuster Nil hint champion Reidrawlctic Crabzzi Sims Breath Hmm........ attendundle\n",
                  "\n",
                  "Target sequence:\n",
                  " great beads. the only reason for the 4 stars...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " too lightweight and they got the quantity wrong\n",
                  "Detokenized output:\n",
                  " hystericalGoalGoalCourSave Release Release contrary Hmm.... gol gol repl Ryu Wad Attacks Breath Milan Senategamescott exposures medicationrank creditcticerve toereportsLetzziitus Linux paletteff pitch Hmm.... ReidMasterMaster vitamins Attacks bowel Attacks Mubcott Release movie138 Linux Information DemandCompany sponsors Shift Sebcott exposures Attacks bowel veggies Breath Senate MubcottUntil\n",
                  "\n",
                  "Epoch: 0, Batch: 0, Loss: 14.420453071594238\n",
                  "Target sequence:\n",
                  " at first i really like this pen style needle felting tool\n",
                  "Detokenized output:\n",
                  "\n",
                  "\n",
                  "Target sequence:\n",
                  " love having a good supply of piping without a bunch...\n",
                  "Detokenized output:\n",
                  "\n",
                  "\n",
                  "Target sequence:\n",
                  " perfect for blocking my knitting projects\n",
                  "Detokenized output:\n",
                  " possibilitiesevil quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality quality\n",
                  "\n",
                  "Target sequence:\n",
                  " just like the old kenyon repair tape.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " plaid paper journey's \"i enjoy being a girl\" paper collection scrapbook.\n",
                  "Detokenized output:\n",
                  "\n",
                  "\n",
                  "Target sequence:\n",
                  " high quality product at a very reasonable price.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " we really wanted it to work properly.....\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " really nice product and fast shipping!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 1, Loss: 36.019901275634766\n",
                  "Target sequence:\n",
                  " i purchased 5 different styles of these to create textured...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " they are okay. do not really work as expected.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " better than medical bench paper - great stuff to use for sewing patterns!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " makes the sewer's life so much simpler\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " hate touching basket in the grocery store\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " feathers came in a good amount of time\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " it might have been my oversight in reading the description...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " marks easily & shows on most my quilting fabrics\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 2, Loss: inf\n",
                  "Target sequence:\n",
                  " love them. must have bought a bazillion of them\n",
                  "Detokenized output:\n",
                  " to! to!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " mirror mirror on the wherever you decide to put this cool paper\n",
                  "Detokenized output:\n",
                  " of to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
                  "\n",
                  "Target sequence:\n",
                  " does not look anything like the picture\n",
                  "Detokenized output:\n",
                  " to! to! to! to!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " love these needles if you have never tried square needles...\n",
                  "Detokenized output:\n",
                  " to to to to to to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to! to!\n",
                  "\n",
                  "Target sequence:\n",
                  "... but i could not wrap up a yarn ball as nicely as all the pictures showed\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " pretty much everything you need to get started\n",
                  "Detokenized output:\n",
                  " to to to to!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " looks like rhinestones, but it is really just light\n",
                  "Detokenized output:\n",
                  " of to!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " so good. they clean up well and i love the...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 3, Loss: 77.66021728515625\n",
                  "Target sequence:\n",
                  " not what was expected; cheap and not worth the money\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " have found some from another supplier that sews better and feels\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " bought this to use for a reward ceremony like when boyscouts/girlscouts earn badges for their uniforms\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " patterns are great, paper not card stock\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " wonder clips make sewing bindings by hand easier\n",
                  "Detokenized output:\n",
                  " it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it\n",
                  "\n",
                  "Target sequence:\n",
                  " bought one for me and one for our daughter when...\n",
                  "Detokenized output:\n",
                  " it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it\n",
                  "\n",
                  "Target sequence:\n",
                  " it is very very hard to put the plastic shortened...\n",
                  "Detokenized output:\n",
                  " it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it it\n",
                  "\n",
                  "Target sequence:\n",
                  " this light board is awesome. my 10 yr old uses it all the...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 4, Loss: 224.30599975585938\n",
                  "Target sequence:\n",
                  " luxurious sewing machine for unlimited creativity\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Target sequence:\n",
                  " wow,, they were delivered so fast,,...\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Target sequence:\n",
                  " these are wonderful for arthric fingers.\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Target sequence:\n",
                  " i bought this hook because i wanted to try tunisian...\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Target sequence:\n",
                  " i hate it and would not have spent the money had...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " great thread, but it is a small spool.\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Target sequence:\n",
                  " should have paid more attention when ordering. i was...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " good material but a lot lighter than what is pictured\n",
                  "Detokenized output:\n",
                  ".........................................................................................................................................................................................................\n",
                  "\n",
                  "Epoch: 0, Batch: 5, Loss: 119.38739013671875\n",
                  "Target sequence:\n",
                  " fabric is not as thick as i hoped for.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " terrific markers at a terrific price\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " i do not like the tweezers though because it is bulky\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " i examine the points under a magnifier and not all...\n",
                  "Detokenized output:\n",
                  " this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this this\n",
                  "\n",
                  "Target sequence:\n",
                  " it is so nice to be able to buy beads that look like...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " this is a great product for wrapping your sofa or seat cushion in\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " it is basic but very sturdy and neat-looking and does an excellent job\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " i am pleased with the ease of assembly and the versatility of...\n",
                  "Detokenized output:\n",
                  " i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
                  "\n",
                  "Epoch: 0, Batch: 6, Loss: 201.70741271972656\n",
                  "Target sequence:\n",
                  " great stuff! definitely going to buy again\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " wonderful purchase experience and quality!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " it works just fine. i was going to get a wooden one...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " love scotties? you will love this calendar!!!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " the recipient was very happy with the gift\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " the reviewers that say this sucks are all speaking of a similar experience to mine\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " it is a nice variety of different styles and shapes\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " good but some tops did not screw right.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 7, Loss: inf\n",
                  "Target sequence:\n",
                  " i bought this to help me outline my photos to watercolor paper\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " great quality and i get beautiful results every time\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " really does keep the skeins separate and they do not...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " i am going out on my very first plein aire journey...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " perfect for artists, very clean edges\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " i am glad i upgraded because i can now use my bigger...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " if you look closely you can tell they are cheaply...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " not as advertised it is only pic no frame lol!!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 8, Loss: inf\n",
                  "Target sequence:\n",
                  " great patch, but beware that it is fairly large\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " the product itself is ok but the picture is pretty deceiving as far as size\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " yarn is great--method of payment very screwed up.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " slotted tool is great... needle tool, not so much\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " love it - gives me so many options for sketching\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " for the price it is not too bad! joints are a bit stiff and can be...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " this ribbon is beautiful. very sturdy and can be shaped very easy\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Target sequence:\n",
                  " great cuitting mat (and mouse pad... see below)\n",
                  "Detokenized output:\n",
                  " for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for for\n",
                  "\n",
                  "Epoch: 0, Batch: 9, Loss: 700.4734497070312\n",
                  "Target sequence:\n",
                  " coat thread and zipper dual duty xp general assortment 50 pack\n",
                  "Detokenized output:\n",
                  " very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very\n",
                  "\n",
                  "Target sequence:\n",
                  " great price, difficult to get started\n",
                  "Detokenized output:\n",
                  " very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "n_epochs = 1\n",
            "batch_size = 8\n",
            "learning_rate = 0.05\n",
            "teacher_forcing_ratio = 0.5\n",
            "hidden_size = 256\n",
            "\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "num_reviwes = len(dataset)\n",
            "n_train_dataset = math.floor(num_reviwes * split_ratios[0])\n",
            "n_validate_dataset = math.floor(num_reviwes * split_ratios[1])\n",
            "n_test_dataset = num_reviwes - n_train_dataset - n_validate_dataset\n",
            "assert n_train_dataset + n_validate_dataset + n_test_dataset == num_reviwes\n",
            "#print(len(dataset))\n",
            "#print(n_train_dataset +n_validate_dataset + n_test_dataset)\n",
            "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [n_train_dataset, n_validate_dataset, n_test_dataset])\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
            "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "batch_sequence = next(iter(train_loader))\n",
            "\n",
            "# Readying the writer\n",
            "writer = SummaryWriter()\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.zeros(dataset.max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "                else:\n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            #print(f\"EOS token found at {target_index}th iteration\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "                \n",
            "train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "3947390f1d33bc3faf6525b63e52b94589ca7e97a4d338993fef3014bbdcaff9"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
