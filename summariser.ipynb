{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 2070 SUPER\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "import os\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# use seaborn style for matplotlib\n",
            "plt.style.use(\"seaborn\")\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "device = th.device(\"cpu\")\n",
            "\n",
            "fig_dir = \"img/\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Original len: 2875917\n",
                  "Pruned len: 1341136\n",
                  "Tokenized len: 1341136\n",
                  "\n",
                  "Max token length of review text:  466\n",
                  "Max token length of summary:  64\n"
               ]
            },
            {
               "data": {
                  "image/png": "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",
                  "text/plain": [
                     "<Figure size 1500x500 with 2 Axes>"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# Use simple GPT2 tokenizer for counting tokens\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "print(f\"Original len: {len(df)}\")\n",
            "df_pruned = util.prune(df)\n",
            "print(f\"Pruned len: {len(df_pruned)}\")\n",
            "util.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "\n",
            "# plot the review and summary word count distribution\n",
            "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
            "\n",
            "text_word_count = df_pruned['reviewText'].apply(lambda x: len(x.split()))\n",
            "summary_word_count = df_pruned['summary'].apply(lambda x: len(x.split()))\n",
            "\n",
            "for ax, data, title in zip(ax.flatten(), [text_word_count, summary_word_count], ['Review Text', 'Summary']):\n",
            "    ax.hist(data, bins=50)\n",
            "    ax.set_title(title)\n",
            "    ax.set_xlabel('Number of words')\n",
            "    ax.set_ylabel('Number of occurrences')\n",
            "\n",
            "plt.show()\n",
            "\n",
            "# save figure as png and pdf\n",
            "fig.savefig(fig_dir + \"review_summary_word_count_distribution.png\", dpi=300)\n",
            "fig.savefig(fig_dir + \"review_summary_word_count_distribution.pdf\")\n",
            "\n",
            "df_tokenized = util.tokenize(df_pruned, tokenizer)\n",
            "print(f\"Tokenized len: {len(df_tokenized)}\")\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df_tokenized['reviewText'].apply(list).apply(len)))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df_tokenized['summary'].apply(list).apply(len))))\n",
            "print(\"Max token length of summary: \", max_summary_len)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "ename": "TypeError",
               "evalue": "object of type 'int' has no len()",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                  "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_tokenized[\u001b[39m\"\u001b[39;49m\u001b[39mreviewText\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlen\u001b[39;49m)\n",
                  "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\polars\\internals\\series\\series.py:3391\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, return_dtype, skip_nulls)\u001b[0m\n\u001b[0;32m   3389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3390\u001b[0m     pl_return_dtype \u001b[39m=\u001b[39m py_type_to_dtype(return_dtype)\n\u001b[1;32m-> 3391\u001b[0m \u001b[39mreturn\u001b[39;00m wrap_s(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_s\u001b[39m.\u001b[39;49mapply_lambda(func, pl_return_dtype, skip_nulls))\n",
                  "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
               ]
            }
         ],
         "source": [
            "# df_tokenized[\"reviewText\"][0]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " cancer and reproduction damage warning label?\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 24753.0, 9288.0, 31452.0, 4742.0, 34035.0, 34055.0, 22907.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage narcisstest Diablodden sketchesusher exacerb\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this product is good, although when i bought it i thought it...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 43562.0, 18633.0, 7810.0, 34055.0, 41729.0, 34055.0, 32287.0, 22647.0, 50003.0, 43789.0, 43789.0, 20563.0, 35238.0, 18112.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage facade broadly colleaguesusherZipusher Knife skate dexteritypackagespackagestheningEEP 127\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " loved the size of this canvas but was super disappointed...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 35562.0, 39432.0, 50003.0, 2466.0, 49699.0, 5683.0, 19543.0, 38686.0, 40541.0, 10814.0, 5845.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage overshadow Checking dexterity**** laced ExtoteriglMaximumHey Those\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 25\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " not a great bottle cutter but good for a one or two project kit, before you decide to buy the expensive cutter...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 24753.0, 24753.0, 27489.0, 13237.0, 329.0, 30095.0, 49735.0, 40914.0, 48573.0, 7566.0, 2587.0, 35831.0, 34055.0, 21949.0, 41729.0, 37107.0, 20490.0, 20490.0, 7269.0, 7269.0, 39432.0, 7566.0, 5085.0, 33551.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage narciss narciss EXTquality for puppet denominationsrx floweringpha material insolusherissueZipigonHumanHuman schedule schedule Checkingpha residents291\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this glue definitely does its job and it is a very great product.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a metal blade held up better. this one has a three position setting\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for blocking projects of any size\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " dries to a tacky finish that can peel right off\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i did not even want to give this product a 1...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 11\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " best fiberfill on the market...by far.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 7168.0, 9235.0, 29120.0, 20039.0, 39432.0, 46652.0, 33551.0, 4408.0, 35791.0, 329.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionageixeligs enzymes predictable Checking bystanders291 scen Trog for\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i did not receive what was in the picture\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 33162.0, 13237.0, 31488.0, 39432.0, 14336.0, 18140.0, 50003.0, 28874.0, 24685.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage astronomersquality luggage Checking Comb remembers dexterityackers fa\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " there was no convenient carrying case\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " envelope would work just fine. otherwise\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 32569.0, 13243.0, 13243.0, 45033.0, 21194.0, 46847.0, 40375.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionagewaves Altern Altern brun gloss aborted attainment\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is a very pretty bracelet. such a high quality look and feel...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great variety of good quality thread colors\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 30113.0, 13404.0, 6187.0, 9050.0, 6187.0, 22647.0, 40914.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage espionage trainsDrITYDr skaterx\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " really like the design of this thing\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 0, Loss: 12.9888277053833\u001b[0m\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " liquitex professional heavy body acrylic paint\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " beautiful. just what i was looking for to display...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 14\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am glad i found this product because you can get a...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i never knew that bamboo hooks could be so easy use. i also loved that they personalized the...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome match for 69 ford instrument cluster needles\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this stuff is the best i have ever had\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 20\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " point protectors and stoppers for knitting needles size #0-10-1/2..\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am right at the end of a project and...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " company was good to work with, the blade did not work well, arrived in a timely fashion.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice beads they r pretty & the holes r 1...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 23\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " these perf3ct scissors have comfortable, roomy handles & can be used right-handed or left-handed\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it knitted up very nicely with much yarn left over to make more\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 24\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  "... together and use it (as the instructions suck) i like it. it had a very unpleasant odor but...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is seriously the biggest waste of money i have experienced in 2017\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " definitely for crafting purposes only. a couple already broke...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good adhesives but the dispensers do not work properly\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 1, Loss: 275.0727233886719\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 14\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " the thread seems to be good quality. i would purchase this again\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it did until we tried applying to the shirts,...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " wonderful product! superfast delivery!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a great way to organize your dp needles\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i have used these exact glue sticks for years. they are great.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 13\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am very pleased with the item as well as the price\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i was sent something totally different. they sent me...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 17\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  "... just starting out cross stitching and these are a great for the variety of colors\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " adorable kit. great customer service.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for the adept and those less so.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 9\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome rotary cutter...works wonders!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " just what i was looking for. great quality, sturdy and easy to work with!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 20\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this thing is sooooooo awesome for practicing my kanji because it is water based no...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i ordered two packages of these string sequins. although...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great when you use it on the appropriate material\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very useful multi-purpose \"fix-all\" substance.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "debugging = True # For debugging prints\n",
            "\n",
            "n_epochs = 10\n",
            "batch_size = 16\n",
            "learning_rate = 0.005\n",
            "teacher_forcing_ratio = 0.5\n",
            "hidden_size = 2**8\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "run_dir = util.get_run_dir()\n",
            "# add run info to the directory\n",
            "run_dir += f\"_batch-size_{batch_size}_learning-rate_{learning_rate}_teacher-forcing-ratio_{teacher_forcing_ratio}_hidden-size_{hidden_size}\"\n",
            "# Readying the writer\n",
            "writer = SummaryWriter(run_dir)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Instantiate tokenizer\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, tokenizer, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "\n",
            "# Get the data loaders\n",
            "train_loader, val_loader, test_loader = util.get_data_loaders(dataset, batch_size, split_ratios)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.float).fill_(t.pad_token_id)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        decoder_output_sequence[target_index] = topi.squeeze().detach() # detach from history as input\n",
            "\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.RED])\n",
            "                    \n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                \n",
            "                if debugging:\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Target tokenized:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(summary.tolist())\n",
            "\n",
            "                    util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    \n",
            "                    util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "            util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\", [util.Modifiers.Colors.BLUE, util.Modifiers.Styles.BOLD])\n",
            "\n",
            "train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "69323fde9fc4d20886c37f6bdc4a05b4e3b82913212d2329f781a907e0bb44ca"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
