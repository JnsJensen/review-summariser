{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/jens/miniconda3/envs/deep-learning/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "cpu\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "#device = th.device(\"cpu\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' def write_dict_to_file(dictionary, file_name):\\n    with open(file_name, \\'w\\') as f:\\n        json.dump(dictionary, f)\\n\\nwrite_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \\n'"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# instantiate roberta tokenizer\n",
            "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
            "\n",
            "# write dictionary to file\n",
            "\"\"\" def write_dict_to_file(dictionary, file_name):\n",
            "    with open(file_name, 'w') as f:\n",
            "        json.dump(dictionary, f)\n",
            "\n",
            "write_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \n",
            "\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\ndf = util.prune(df)\\nutil.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\\ndf = util.tokenize(df, tokenizer)\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df[\\'reviewText\\'].apply(list).apply(len)))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df[\\'summary\\'].apply(list).apply(len))))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Use simple GPT2 tokenizer for counting tokens\n",
            "\"\"\" tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "df = util.prune(df)\n",
            "util.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "df = util.tokenize(df, tokenizer)\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df['reviewText'].apply(list).apply(len)))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df['summary'].apply(list).apply(len))))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " best ruler i have used!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 25282.0, 86.0, 40863.0, 45791.0, 34275.0, 10884.0, 10884.0, 42603.0, 42603.0, 42780.0, 45254.0, 45254.0, 42780.0, 48343.0, 48343.0, 48343.0, 41891.0, 48343.0, 48343.0, 48343.0, 48343.0, 42780.0, 42780.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 42780.0, 48343.0, 12839.0, 48343.0, 48343.0, 42780.0, 42780.0, 42780.0, 12839.0, 12839.0, 43915.0, 11378.0, 26002.0, 42780.0, 18062.0, 42780.0, 48343.0, 42780.0, 41891.0, 45707.0, 42780.0, 48343.0, 13549.0, 12839.0, 12839.0, 42780.0, 42780.0, 12839.0, 42780.0, 42780.0, 48343.0, 48343.0, 48343.0, 42780.0, 42780.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Bundlew Jaw663 hunted Sin Sin Blackburn Blackburn426 dorsal dorsal426okinglyokinglyokinglypickedokinglyokinglyokinglyokingly426426okinglyokinglyokinglyokinglyokinglyokinglyokinglyokingly426okingly therebyokinglyokingly426426426 thereby thereby Chung satisfied praying426 Verizon426okingly426picked impecc426okingly Horn thereby thereby426426 thereby426426okinglyokinglyokingly426426\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " pricey for the quality of tools\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 24422.0, 24422.0, 24422.0, 24422.0, 31618.0, 34492.0, 31618.0, 34492.0, 47046.0, 39757.0, 17574.0, 40681.0, 28494.0, 11586.0, 16004.0, 16004.0, 781.0, 36032.0, 19975.0, 15043.0, 48929.0, 48170.0, 48170.0, 44076.0, 35767.0, 6691.0, 17303.0, 31618.0, 40795.0, 12334.0, 42603.0, 31618.0, 42603.0, 31618.0, 42603.0, 31618.0, 40795.0, 42603.0, 31618.0, 34492.0, 45990.0, 31618.0, 40795.0, 23225.0, 42603.0, 31618.0, 42603.0, 31618.0, 34492.0, 39757.0, 40681.0, 40250.0, 41891.0, 6995.0, 11586.0, 16004.0, 21681.0, 48398.0, 5886.0, 5886.0, 20435.0, 46346.0, 1194.0, 8603.0, 29664.0, 9444.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Requirements Requirements Requirements Requirements Parkinson Braun Parkinson Braun GABAchannelAvailabilityFoot racists duct Anim Hurricane Hurricane fl Riding indictment permanentlygru517517inelliPinterestatures Armen Parkinson PAL ow Blackburn Parkinson Blackburn Parkinson Blackburn Parkinson PAL Blackburn Parkinson Braunˈ Parkinson PALelli Blackburn Parkinson Blackburn Parkinson BraunchannelAvailability racists waterfrontpicked Houston Anim Hurricanelar unintentionalOverOver coordinate vacated another equallywolvesCons\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for snowglobes!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 24422.0, 24422.0, 24422.0, 24422.0, 24422.0, 31618.0, 34492.0, 31618.0, 34492.0, 47046.0, 31618.0, 3703.0, 39757.0, 17574.0, 25272.0, 208.0, 46986.0, 27574.0, 23386.0, 9124.0, 17303.0, 48170.0, 48170.0, 9670.0, 37610.0, 36244.0, 8519.0, 45340.0, 10884.0, 25554.0, 19975.0, 2670.0, 1717.0, 45340.0, 1310.0, 17281.0, 8603.0, 9116.0, 31618.0, 31618.0, 42603.0, 31618.0, 34492.0, 39757.0, 45990.0, 31618.0, 42603.0, 31618.0, 42603.0, 31618.0, 34492.0, 34492.0, 39757.0, 38445.0, 44899.0, 19904.0, 8603.0, 42603.0, 15327.0, 8603.0, 43322.0, 25554.0, 19975.0, 45771.0, 9491.0, 15428.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Requirements Requirements Requirements Requirements Requirements Parkinson Braun Parkinson Braun GABA Parkinson detailchannelAvailabilityFoot196\u0014 Technician Eat Jadeped Armen517517 lucky impunity311 Disney+++ Sin Yug indictment39 Ed+++ little Harrison equallyü Parkinson Parkinson Blackburn Parkinson BraunchannelAvailabilityˈ Parkinson Blackburn Parkinson Blackburn Parkinson Braun BraunchannelAvailability geometricCHOInv equally Blackburn\"— equally Liang Yug indictment airflowjarwaukee\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it took forever for the ink to come down,...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 24422.0, 24422.0, 24422.0, 31618.0, 24422.0, 31618.0, 34492.0, 31618.0, 34492.0, 39757.0, 17574.0, 40681.0, 43962.0, 7377.0, 12248.0, 31618.0, 42603.0, 31618.0, 40795.0, 34275.0, 42603.0, 6906.0, 31618.0, 42603.0, 31618.0, 34492.0, 39757.0, 45990.0, 1310.0, 36244.0, 30518.0, 43322.0, 8603.0, 42603.0, 31618.0, 42603.0, 42690.0, 42603.0, 2482.0, 32290.0, 208.0, 29154.0, 208.0, 33845.0, 28886.0, 29154.0, 18044.0, 16560.0, 34275.0, 33932.0, 42603.0, 31618.0, 8263.0, 36244.0, 19975.0, 8603.0, 31102.0, 44451.0, 12311.0, 36244.0, 9491.0, 5689.0, 25554.0, 19975.0, 45771.0, 49497.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Requirements Requirements Requirements Parkinson Requirements Parkinson Braun Parkinson BraunchannelAvailabilityFoot racists pokemon �?! Parkinson Blackburn Parkinson PAL hunted Blackburn depending Parkinson Blackburn Parkinson BraunchannelAvailabilityˈ little311 seism Liang equally Blackburn Parkinson Blackburnalien Blackburn results showers\u0014 Knowing\u0014 iPhonesDNA Knowing breathe incorporated hunted cavern Blackburn Parkinson drawing311 indictment equally640raughtieve311jar Joe Yug indictment airflow torches\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but it is super inconvenient.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 35530.0, 86.0, 24422.0, 24507.0, 27412.0, 45254.0, 17303.0, 33346.0, 12839.0, 48343.0, 45707.0, 42780.0, 42780.0, 42780.0, 42780.0, 45254.0, 11378.0, 11378.0, 23972.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 11378.0, 42603.0, 48343.0, 48343.0, 48343.0, 48343.0, 42780.0, 18062.0, 42603.0, 42603.0, 48343.0, 48343.0, 42603.0, 42603.0, 43915.0, 42780.0, 42780.0, 12839.0, 42780.0, 42780.0, 41891.0, 42780.0, 42780.0, 48343.0, 48343.0, 11378.0, 11378.0, 23972.0, 43915.0, 43915.0, 18173.0, 42780.0, 48343.0, 48343.0, 42780.0, 42780.0, 42780.0, 12839.0, 42780.0, 42780.0, 48343.0, 43915.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Communitiesw Requirements Blind368 dorsal ArmenGPU therebyokingly impecc426426426426 dorsal satisfied satisfied tomatoesokinglyokinglyokinglyokinglyokingly satisfied Blackburnokinglyokinglyokinglyokingly426 Verizon Blackburn Blackburnokinglyokingly Blackburn Blackburn Chung426426 thereby426426picked426426okinglyokingly satisfied satisfied tomatoes Chung Chunguler426okinglyokingly426426426 thereby426426okingly Chung\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good, worked, easy to use\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[25282.0, 35530.0, 19668.0, 8132.0, 24422.0, 24422.0, 4859.0, 12839.0, 15807.0, 12839.0, 12839.0, 12839.0, 12839.0, 41891.0, 48343.0, 48343.0, 48343.0, 15914.0, 42603.0, 42780.0, 48343.0, 48343.0, 41891.0, 24422.0, 12839.0, 12839.0, 43915.0, 18173.0, 48343.0, 48343.0, 48343.0, 48343.0, 42780.0, 48343.0, 48343.0, 48343.0, 48343.0, 24422.0, 42780.0, 12839.0, 42780.0, 12839.0, 12839.0, 12839.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 18062.0, 13549.0, 13549.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 48343.0, 12839.0, 48343.0, 42780.0, 42780.0, 48343.0, 48343.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Bundle Communitiesassetsrance Requirements Requirements stick thereby fortune thereby thereby thereby therebypickedokinglyokinglyokinglyfootball Blackburn426okinglyokinglypicked Requirements thereby thereby Chungulerokinglyokinglyokinglyokingly426okinglyokinglyokinglyokingly Requirements426 thereby426 thereby thereby therebyokinglyokinglyokinglyokinglyokingly Verizon Horn Hornokinglyokinglyokinglyokinglyokinglyokinglyokinglyokinglyokingly therebyokingly426426okinglyokingly\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 0, Loss: 21.63437843322754\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " going crazy looking for this and aha!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 2\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " lots of paper.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " draw whatever your heart desires on t-shirts and other fabric items!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 2\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great purchase\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great set\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 2\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great double side tape that is hard to find at arts and crafts stores.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n"
               ]
            },
            {
               "ename": "",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31mCanceled future for execute_request message before replies were done"
               ]
            },
            {
               "ename": "",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "debugging = True\n",
            "\n",
            "n_epochs = 10\n",
            "batch_size = 6\n",
            "learning_rate = 0.001\n",
            "teacher_forcing_ratio = 0.5\n",
            "#hidden_size = 2**9 # 512\n",
            "hidden_size = 2**10\n",
            "\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "num_reviwes = len(dataset)\n",
            "n_train_dataset = math.floor(num_reviwes * split_ratios[0])\n",
            "n_validate_dataset = math.floor(num_reviwes * split_ratios[1])\n",
            "n_test_dataset = num_reviwes - n_train_dataset - n_validate_dataset\n",
            "assert n_train_dataset + n_validate_dataset + n_test_dataset == num_reviwes\n",
            "#print(len(dataset))\n",
            "#print(n_train_dataset +n_validate_dataset + n_test_dataset)\n",
            "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [n_train_dataset, n_validate_dataset, n_test_dataset])\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
            "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "batch_sequence = next(iter(train_loader))\n",
            "\n",
            "# Readying the writer\n",
            "writer = SummaryWriter()\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.float).fill_(t.pad_token_id)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        decoder_output_sequence[target_index] = topi.squeeze().detach() # detach from history as input\n",
            "\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.RED])\n",
            "                    \n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                \n",
            "                if debugging:\n",
            "                    util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    \n",
            "                    util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "            util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\", [util.Modifiers.Colors.BLUE, util.Modifiers.Styles.BOLD])\n",
            "\n",
            "train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3.10.8 ('deep-learning')",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "c97f19f1a61f7391961f30397493e1a2688eb0342e378cc602641384d76195b8"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
