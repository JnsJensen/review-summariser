{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "mps\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "device = th.device(\"cpu\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' def write_dict_to_file(dictionary, file_name):\\n    with open(file_name, \\'w\\') as f:\\n        json.dump(dictionary, f)\\n\\nwrite_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \\n'"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# instantiate roberta tokenizer\n",
            "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
            "\n",
            "# write dictionary to file\n",
            "\"\"\" def write_dict_to_file(dictionary, file_name):\n",
            "    with open(file_name, 'w') as f:\n",
            "        json.dump(dictionary, f)\n",
            "\n",
            "write_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \n",
            "\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\ndf = util.prune(df)\\nutil.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\\ndf = util.tokenize(df, tokenizer)\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df[\\'reviewText\\'].apply(list).apply(len)))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df[\\'summary\\'].apply(list).apply(len))))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Use simple GPT2 tokenizer for counting tokens\n",
            "\"\"\" tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "df = util.prune(df)\n",
            "util.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "df = util.tokenize(df, tokenizer)\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df['reviewText'].apply(list).apply(len)))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df['summary'].apply(list).apply(len))))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " cancer and reproduction damage warning label?\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 24753.0, 9288.0, 31452.0, 4742.0, 34035.0, 34055.0, 22907.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage narcisstest Diablodden sketchesusher exacerb\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this product is good, although when i bought it i thought it...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 43562.0, 18633.0, 7810.0, 34055.0, 41729.0, 34055.0, 32287.0, 22647.0, 50003.0, 43789.0, 43789.0, 20563.0, 35238.0, 18112.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage facade broadly colleaguesusherZipusher Knife skate dexteritypackagespackagestheningEEP 127\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " loved the size of this canvas but was super disappointed...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 35562.0, 39432.0, 50003.0, 2466.0, 49699.0, 5683.0, 19543.0, 38686.0, 40541.0, 10814.0, 5845.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage overshadow Checking dexterity**** laced ExtoteriglMaximumHey Those\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 25\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " not a great bottle cutter but good for a one or two project kit, before you decide to buy the expensive cutter...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 24753.0, 24753.0, 27489.0, 13237.0, 329.0, 30095.0, 49735.0, 40914.0, 48573.0, 7566.0, 2587.0, 35831.0, 34055.0, 21949.0, 41729.0, 37107.0, 20490.0, 20490.0, 7269.0, 7269.0, 39432.0, 7566.0, 5085.0, 33551.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage narciss narciss EXTquality for puppet denominationsrx floweringpha material insolusherissueZipigonHumanHuman schedule schedule Checkingpha residents291\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this glue definitely does its job and it is a very great product.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a metal blade held up better. this one has a three position setting\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for blocking projects of any size\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " dries to a tacky finish that can peel right off\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i did not even want to give this product a 1...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 11\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " best fiberfill on the market...by far.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 7168.0, 9235.0, 29120.0, 20039.0, 39432.0, 46652.0, 33551.0, 4408.0, 35791.0, 329.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionageixeligs enzymes predictable Checking bystanders291 scen Trog for\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i did not receive what was in the picture\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 33162.0, 13237.0, 31488.0, 39432.0, 14336.0, 18140.0, 50003.0, 28874.0, 24685.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage astronomersquality luggage Checking Comb remembers dexterityackers fa\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " there was no convenient carrying case\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " envelope would work just fine. otherwise\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 32569.0, 13243.0, 13243.0, 45033.0, 21194.0, 46847.0, 40375.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionagewaves Altern Altern brun gloss aborted attainment\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is a very pretty bracelet. such a high quality look and feel...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great variety of good quality thread colors\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 30113.0, 30113.0, 13404.0, 6187.0, 9050.0, 6187.0, 22647.0, 40914.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribe espionage espionage trainsDrITYDr skaterx\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " really like the design of this thing\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[19808.0, 12368.0, 16509.0, 35815.0, 4010.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 13585.0, 50003.0, 50003.0, 33367.0, 36840.0, 2587.0, 49588.0, 329.0, 24685.0, 44942.0, 28331.0, 28331.0, 22647.0, 4742.0, 28874.0, 11955.0, 39777.0, 28874.0, 35613.0, 11478.0, 13243.0, 17951.0, 329.0, 24685.0, 14963.0, 49448.0, 49448.0, 6924.0, 42669.0, 20192.0, 35921.0, 22907.0, 33551.0, 38635.0, 36146.0, 35613.0, 22423.0, 12188.0, 36563.0, 21194.0, 22907.0, 12188.0, 38686.0, 34055.0, 49448.0, 49448.0, 49448.0, 6924.0, 31510.0, 9578.0, 22907.0, 33551.0, 38635.0, 36146.0, 4010.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  " Subscribeeller Shoilus challeng Fac dexterity dexterity blahLOCK material Geh Fac dexterity dexterity blahLOCK material Geh for faendars famed famed skateddenackers Gre rookiesackersLOAD transmission Alterniked for fa occurring sheltered sheltered medic).[border Boxing exacerb291 irritated improvisedLOAD ratios builds]); gloss exacerb buildsiglusher sheltered sheltered sheltered medic 226 Left exacerb291 irritated improvised challeng\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 0, Loss: 12.9888277053833\u001b[0m\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " liquitex professional heavy body acrylic paint\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " beautiful. just what i was looking for to display...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 14\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am glad i found this product because you can get a...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i never knew that bamboo hooks could be so easy use. i also loved that they personalized the...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome match for 69 ford instrument cluster needles\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this stuff is the best i have ever had\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 20\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " point protectors and stoppers for knitting needles size #0-10-1/2..\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am right at the end of a project and...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " company was good to work with, the blade did not work well, arrived in a timely fashion.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice beads they r pretty & the holes r 1...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 23\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " these perf3ct scissors have comfortable, roomy handles & can be used right-handed or left-handed\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it knitted up very nicely with much yarn left over to make more\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 24\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  "... together and use it (as the instructions suck) i like it. it had a very unpleasant odor but...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is seriously the biggest waste of money i have experienced in 2017\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " definitely for crafting purposes only. a couple already broke...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 1\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good adhesives but the dispensers do not work properly\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 1, Loss: 275.0727233886719\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 14\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " the thread seems to be good quality. i would purchase this again\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it did until we tried applying to the shirts,...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 8\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " wonderful product! superfast delivery!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a great way to organize your dp needles\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 15\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i have used these exact glue sticks for years. they are great.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 13\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am very pleased with the item as well as the price\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 12\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i was sent something totally different. they sent me...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 17\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  "... just starting out cross stitching and these are a great for the variety of colors\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " adorable kit. great customer service.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for the adept and those less so.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 9\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome rotary cutter...works wonders!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " just what i was looking for. great quality, sturdy and easy to work with!\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 20\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this thing is sooooooo awesome for practicing my kanji because it is water based no...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i ordered two packages of these string sequins. although...\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 10\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great when you use it on the appropriate material\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 6\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very useful multi-purpose \"fix-all\" substance.\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50257.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "debugging = True\n",
            "\n",
            "n_epochs = 10\n",
            "batch_size = 16\n",
            "learning_rate = 0.005\n",
            "teacher_forcing_ratio = 0.5\n",
            "#hidden_size = 2**9 # 512\n",
            "hidden_size = 2**9\n",
            "\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "num_reviwes = len(dataset)\n",
            "n_train_dataset = math.floor(num_reviwes * split_ratios[0])\n",
            "n_validate_dataset = math.floor(num_reviwes * split_ratios[1])\n",
            "n_test_dataset = num_reviwes - n_train_dataset - n_validate_dataset\n",
            "assert n_train_dataset + n_validate_dataset + n_test_dataset == num_reviwes\n",
            "#print(len(dataset))\n",
            "#print(n_train_dataset +n_validate_dataset + n_test_dataset)\n",
            "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [n_train_dataset, n_validate_dataset, n_test_dataset])\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
            "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "batch_sequence = next(iter(train_loader))\n",
            "\n",
            "# Readying the writer\n",
            "writer = SummaryWriter()\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.float).fill_(t.pad_token_id)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        decoder_output_sequence[target_index] = topi.squeeze().detach() # detach from history as input\n",
            "\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.RED])\n",
            "                    \n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                \n",
            "                if debugging:\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Target tokenized:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(summary.tolist())\n",
            "\n",
            "                    util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    \n",
            "                    util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "            util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\", [util.Modifiers.Colors.BLUE, util.Modifiers.Styles.BOLD])\n",
            "\n",
            "train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "pytorch1.13",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.6"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "74ce01c583b9da8e861aa1ee054dcef8ec2fe05bc1a1578aa65fd6e69a36df1a"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
