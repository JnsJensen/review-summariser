{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "import re\n",
    "from enum import Enum\n",
    "\n",
    "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(th.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"datasets/\"\n",
    "\n",
    "# Dataset paths\n",
    "data_original = data_dir + \"full/\" # Use this for full dataset that contains all review matadata\n",
    "data_pruned = data_dir + \"pruned/\" # Only datasets with one number to the right side of it have a pruned version\n",
    "data_tokenized = data_dir + \"tokenized/\" # Only datasets with two numbers to the right side of it have a tokenized version\n",
    "\n",
    "full = \"All_Amazon_Review_5\" # 80 GB\n",
    "arts = \"Arts_Crafts_and_Sewing\" # 518 MB / 629 MB / 1.18 GB\n",
    "video = \"Amazon_Instant_Video_5\" # 28 MB\n",
    "gift = \"Gift_Cards_5\" # 0.88 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    ORIGINAL = 0\n",
    "    PRUNED = 1\n",
    "    TOKENIZED = 2\n",
    "\n",
    "def prune(df: pd.DataFrame | pl.DataFrame) -> pl.DataFrame:\n",
    "    # list of unwanted summaries in lower case\n",
    "    summary_filter = [\"one star\", \"two stars\", \"three stars\", \"fours stars\", \"five stars\"]\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = pl.DataFrame(df.dropna())\n",
    "    \n",
    "    assert(isinstance(df, pl.DataFrame))\n",
    "\n",
    "    df = df.filter(pl.col(\"summary\").apply(str.lower) != \"five stars\") \\\n",
    "           .filter(pl.col(\"summary\").apply(str.lower) != \"four stars\") \\\n",
    "           .filter(pl.col(\"summary\").apply(str.lower) != \"three stars\") \\\n",
    "           .filter(pl.col(\"summary\").apply(str.lower) != \"two stars\") \\\n",
    "           .filter(pl.col(\"summary\").apply(str.lower) != \"one star\")\n",
    "    return df\n",
    "\n",
    "def write_to_csv(df: pd.DataFrame | pl.DataFrame, path: str) -> None:\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df.to_csv(path + \".csv\", index=False)\n",
    "    elif isinstance(df, pl.DataFrame):\n",
    "        df.write_csv(path + \".csv\")\n",
    "\n",
    "def tokenize(df: pl.DataFrame, tokenizer: GPT2Tokenizer) -> pl.DataFrame:\n",
    "    t = lambda x: tokenizer.encode(x, add_special_tokens=True)\n",
    "    df = df.lazy().select([\n",
    "        pl.col(\"reviewText\").apply(t),\n",
    "        pl.col(\"summary\").apply(t),\n",
    "        pl.exclude([\"reviewText\", \"summary\"])\n",
    "    ]).collect()\n",
    "    return df\n",
    "\n",
    "def load_dataset(dataset: str, dataset_type: DatasetType, keep_cols = [\"reviewText\", \"summary\", \"overall\"]) -> pd.DataFrame | pl.DataFrame:\n",
    "    if dataset_type == DatasetType.ORIGINAL:\n",
    "        # return pl.read_json(data_original + dataset + \".json\", json_lines=True).select([keep_cols])\n",
    "        return pd.read_json(data_original + dataset + \".json\", lines=True)[keep_cols]\n",
    "    elif dataset_type == DatasetType.PRUNED:\n",
    "        return pl.read_csv(data_pruned + dataset + \".csv\", dtypes={\"reviewtext\": pl.Utf8, \"summary\": pl.Utf8, \"overall\": pl.Int8})\n",
    "    elif dataset_type == DatasetType.TOKENIZED:\n",
    "        df = pl.read_csv(data_tokenized + dataset + \".csv\", dtypes={\"reviewtext\": pl.Utf8, \"summary\": pl.Utf8, \"overall\": pl.Int8})\n",
    "        listify = lambda x: x.split(\"|\")\n",
    "        df = df.lazy().select([\n",
    "            pl.col(\"reviewText\").apply(listify).cast(pl.List(pl.Int64)),\n",
    "            pl.col(\"summary\").apply(listify).cast(pl.List(pl.Int64)),\n",
    "            pl.exclude([\"reviewText\", \"summary\"])\n",
    "        ]).collect()\n",
    "        return df\n",
    "\n",
    "def save_dataset(df: pl.DataFrame, dataset: str, dataset_type: DatasetType) -> None:\n",
    "    if dataset_type == DatasetType.PRUNED:\n",
    "        write_to_csv(df, data_pruned + dataset)\n",
    "    elif dataset_type == DatasetType.TOKENIZED:\n",
    "        stringify = lambda x: \"|\".join(list(x.cast(pl.Utf8)))\n",
    "        df = df.lazy().select([\n",
    "            pl.col(\"reviewText\").apply(stringify),\n",
    "            pl.col(\"summary\").apply(stringify),\n",
    "            pl.exclude([\"reviewText\", \"summary\"])\n",
    "        ]).collect()\n",
    "        write_to_csv(df, data_tokenized + dataset)\n",
    "\n",
    "def preprocess(dataset: str, dataset_type: DatasetType, tokenizer, keep_cols = [\"reviewText\", \"summary\", \"overall\"], save_steps=True) -> pd.DataFrame | pl.DataFrame:\n",
    "    if dataset_type == DatasetType.ORIGINAL:\n",
    "        df = load_dataset(dataset, dataset_type, keep_cols)\n",
    "        df = prune(df)\n",
    "        if save_steps:\n",
    "            save_dataset(df, dataset, DatasetType.PRUNED)\n",
    "        df = tokenize(df, tokenizer)\n",
    "        if save_steps:\n",
    "            save_dataset(df, dataset, DatasetType.TOKENIZED)\n",
    "        return df\n",
    "    elif dataset_type == DatasetType.PRUNED:\n",
    "        df = load_dataset(dataset, dataset_type)\n",
    "        df = tokenize(df, tokenizer)\n",
    "        if save_steps:\n",
    "            save_dataset(df, dataset, DatasetType.TOKENIZED)\n",
    "        return df\n",
    "    elif dataset_type == DatasetType.TOKENIZED:\n",
    "        df = load_dataset(dataset, dataset_type)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune and save\n",
    "df = load_dataset(arts, DatasetType.ORIGINAL)\n",
    "df = prune(df)\n",
    "save_dataset(df, arts, DatasetType.PRUNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I received these earlier than expected. Have requested a refund because I received a box of 12 gold markers and I was hoping to receive 12 colors.\n",
      "One color\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 3)\n",
      "┌─────────────────────────┬───────────────────────┬─────────┐\n",
      "│ reviewText              ┆ summary               ┆ overall │\n",
      "│ ---                     ┆ ---                   ┆ ---     │\n",
      "│ list[i64]               ┆ list[i64]             ┆ i8      │\n",
      "╞═════════════════════════╪═══════════════════════╪═════════╡\n",
      "│ [10248, 3081, ... 6930] ┆ [10248, 1988]         ┆ 5       │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "│ [4711, 389, ... 13]     ┆ [4711, 389, ... 6546] ┆ 4       │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "│ [464, 29707, ... 13]    ┆ [986, 1718, ... 13]   ┆ 3       │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "│ [38052, 777, ... 3228]  ┆ [13681, 1720, 3228]   ┆ 5       │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
      "│ [40, 2722, ... 13]      ┆ [3198, 3124]          ┆ 1       │\n",
      "└─────────────────────────┴───────────────────────┴─────────┘\n",
      "40|1053|1100|428|1492|1541|290|314|1053|1392|3352|329|1262|340|287|2003|4493|13|220|314|1101|28163|9947|1961|351|262|7572|287|340|290|262|5608|290|11776|389|655|355|922|355|345|561|1607|422|24164|33927|805|13|220|314|1101|523|9675|326|314|5839|428|13|220|1081|257|25837|290|28357|638|1967|11|428|468|587|257|8119|3090|284|616|1541|922|19943|1492|4947|13|220|6930|24164|329|428|845|2041|41577|2190|13\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and save\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "df = load_dataset(arts, DatasetType.PRUNED)\n",
    "print(df[\"reviewText\"][-1])\n",
    "print(df[\"summary\"][-1])\n",
    "print(df[\"overall\"][-1])\n",
    "\n",
    "df = tokenize(df, tokenizer)\n",
    "print(df.tail())\n",
    "\n",
    "print(\"|\".join(list(df[\"reviewText\"][0].cast(pl.Utf8))))\n",
    "save_dataset(df, arts, DatasetType.TOKENIZED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        white-space: pre;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-top: 0;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-bottom: 0;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\" >\n",
       "<small>shape: (5, 3)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "reviewText\n",
       "</th>\n",
       "<th>\n",
       "summary\n",
       "</th>\n",
       "<th>\n",
       "overall\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "list[i64]\n",
       "</td>\n",
       "<td>\n",
       "list[i64]\n",
       "</td>\n",
       "<td>\n",
       "i8\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "[10248, 3081, ... 6930]\n",
       "</td>\n",
       "<td>\n",
       "[10248, 1988]\n",
       "</td>\n",
       "<td>\n",
       "5\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "[4711, 389, ... 13]\n",
       "</td>\n",
       "<td>\n",
       "[4711, 389, ... 6546]\n",
       "</td>\n",
       "<td>\n",
       "4\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "[464, 29707, ... 13]\n",
       "</td>\n",
       "<td>\n",
       "[986, 1718, ... 13]\n",
       "</td>\n",
       "<td>\n",
       "3\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "[38052, 777, ... 3228]\n",
       "</td>\n",
       "<td>\n",
       "[13681, 1720, 3228]\n",
       "</td>\n",
       "<td>\n",
       "5\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "[40, 2722, ... 13]\n",
       "</td>\n",
       "<td>\n",
       "[3198, 3124]\n",
       "</td>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────────────────┬───────────────────────┬─────────┐\n",
       "│ reviewText              ┆ summary               ┆ overall │\n",
       "│ ---                     ┆ ---                   ┆ ---     │\n",
       "│ list[i64]               ┆ list[i64]             ┆ i8      │\n",
       "╞═════════════════════════╪═══════════════════════╪═════════╡\n",
       "│ [10248, 3081, ... 6930] ┆ [10248, 1988]         ┆ 5       │\n",
       "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
       "│ [4711, 389, ... 13]     ┆ [4711, 389, ... 6546] ┆ 4       │\n",
       "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
       "│ [464, 29707, ... 13]    ┆ [986, 1718, ... 13]   ┆ 3       │\n",
       "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
       "│ [38052, 777, ... 3228]  ┆ [13681, 1720, 3228]   ┆ 5       │\n",
       "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n",
       "│ [40, 2722, ... 13]      ┆ [3198, 3124]          ┆ 1       │\n",
       "└─────────────────────────┴───────────────────────┴─────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenized data\n",
    "# df = load_dataset(arts, DatasetType.TOKENIZED)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# df = load_dataset(arts, DatasetType.TOKENIZED)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# Find max length of review text with numpy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m max_review_len \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmax(\u001b[39mlist\u001b[39;49m(df[\u001b[39m'\u001b[39;49m\u001b[39mreviewText\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlist\u001b[39;49m)\u001b[39m.\u001b[39;49mapply(\u001b[39mlen\u001b[39;49m)))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMax length of review text: \u001b[39m\u001b[39m\"\u001b[39m, max_review_len)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Find max length of summary with numpy\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# df = load_dataset(arts, DatasetType.TOKENIZED)\n",
    "\n",
    "# Find max length of review text with numpy\n",
    "max_review_len = np.max(list(df['reviewText'].apply(list).apply(len)))\n",
    "print(\"\\nMax length of review text: \", max_review_len)\n",
    "# Find max length of summary with numpy\n",
    "max_summary_len = np.max((list(df['summary'].apply(list).apply(len))))\n",
    "print(\"Max length of summary: \", max_summary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1535558 has no review tokens\n",
    "# summary: 1983|4846|317|321|343|29375|10318|282|1689|12|978|32|283|312|71|978|4627|259|993|978|29856|707|86|3301|5598|2623|532|49125|1415|7526|7420|9671\n",
    "# detokenize, find which data point it in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# df = preprocess(arts, DatasetType.ORIGINAL, tokenizer)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# Plot a distribution of review lengths with log scale\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m r_counts, r_bins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhistogram(df[\u001b[39m\"\u001b[39;49m\u001b[39mreviewText\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlen\u001b[39;49m), bins\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m s_counts, s_bins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhistogram(df[\u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlen\u001b[39m), bins\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m      9\u001b[0m r_vcounts \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mreviewText\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlen\u001b[39m)\u001b[39m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/histograms.py:793\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \n\u001b[1;32m    790\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m a, weights \u001b[39m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 793\u001b[0m bin_edges, uniform_bins \u001b[39m=\u001b[39m _get_bin_edges(a, bins, \u001b[39mrange\u001b[39;49m, weights)\n\u001b[1;32m    795\u001b[0m \u001b[39m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mif\u001b[39;00m n_equal_bins \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`bins` must be positive, when an integer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[39m=\u001b[39m _get_outer_edges(a, \u001b[39mrange\u001b[39;49m)\n\u001b[1;32m    428\u001b[0m \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39mndim(bins) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mmin(), a\u001b[39m.\u001b[39mmax()\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (np\u001b[39m.\u001b[39misfinite(first_edge) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mautodetected range of [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m] is not finite\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[39m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m first_edge \u001b[39m==\u001b[39m last_edge:\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = preprocess(arts, DatasetType.ORIGINAL, tokenizer)\n",
    "\n",
    "# Plot a distribution of review lengths with log scale\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "r_counts, r_bins = np.histogram(df[\"reviewText\"].apply(len), bins=100)\n",
    "s_counts, s_bins = np.histogram(df[\"summary\"].apply(len), bins=100)\n",
    "\n",
    "r_vcounts = df[\"reviewText\"].apply(len).value_counts()\n",
    "s_vcounts = df[\"summary\"].apply(len).value_counts()\n",
    "r_vbins = list(r_vcounts[\"reviewText\"])\n",
    "s_vbins = list(s_vcounts[\"summary\"])\n",
    "\n",
    "log_yscale = True\n",
    "\n",
    "ax[0].hist(df[\"reviewText\"].apply(len), bins=range(np.min(r_vbins), np.max(r_vbins)), log=log_yscale)\n",
    "ax[0].set_title(\"Distribution of review lengths\")\n",
    "ax[0].set_xlabel(\"Review length\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].hist(df[\"summary\"].apply(len), bins=range(np.min(s_vbins), np.max(s_vbins)), log=log_yscale)\n",
    "ax[1].set_title(\"Distribution of summary lengths\")\n",
    "ax[1].set_xlabel(\"summary length\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset from pandas dataframe\n",
    "# defines a voacbulary of words and converts the review text to a list of indices\n",
    "# beware of symbols like ., !, ? etc.\n",
    "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
    "\n",
    "class ReviewDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.df.iloc[idx, 0].split()\n",
    "        review = [self.vocab2idx[word] for word in review]\n",
    "        review = th.tensor(review, dtype=th.long)\n",
    "        review = th.nn.functional.pad(review, (0, max_review_len - len(review)))\n",
    "        rating = self.df.iloc[idx, 1]\n",
    "        rating = th.tensor(rating, dtype=th.long)\n",
    "        summary = self.df.iloc[idx, 2].split()\n",
    "        summary = [self.vocab2idx[word] for word in summary]\n",
    "        summary = th.tensor(summary, dtype=th.long)\n",
    "        summary = th.nn.functional.pad(summary, (0, max_summary_len - len(summary)))\n",
    "\n",
    "        # move tensors to device\n",
    "        review = review.to(device)\n",
    "        rating = rating.to(device)\n",
    "        summary = summary.to(device)\n",
    "        \n",
    "        return review, rating, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset from pandas dataframe\n",
    "# defines a voacbulary of words and converts the review text to a list of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dataset \u001b[39m=\u001b[39m ReviewDataset(df)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(dataset[\u001b[39m0\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn [7], line 37\u001b[0m, in \u001b[0;36mReviewDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m review \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(review, (\u001b[39m0\u001b[39m, max_review_len \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(review)))\n\u001b[1;32m     36\u001b[0m rating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[idx, \u001b[39m1\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m rating \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39;49mtensor(rating, dtype\u001b[39m=\u001b[39;49mth\u001b[39m.\u001b[39;49mlong)\n\u001b[1;32m     38\u001b[0m summary \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[idx, \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     39\u001b[0m summary \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab2idx[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m summary]\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# test the dataset\n",
    "dataset = ReviewDataset(df)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model\n",
    "uses context aware word embedding\n",
    "multi-task network\n",
    "\n",
    "Input: takes in a review string\n",
    "Task 1: output a summary string of the input review with a max length defined by the dataset\n",
    "Task 2: output a rating of the input review as a float 0-1\n",
    "\n",
    "Use an encoder decoder setup with one decoder for each task\n",
    "\"\"\"\n",
    "class Summariser(th.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_review_len, max_summary_len):\n",
    "        super(Summariser, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_review_len = max_review_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "        self.embedding = th.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.decoder1 = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True)\n",
    "        self.decoder2 = th.nn.LSTM(embedding_dim, embedding_dim, num_layers=2, batch_first=True)\n",
    "        self.linear1 = th.nn.Linear(embedding_dim, vocab_size)\n",
    "        self.linear2 = th.nn.Linear(embedding_dim, 1)\n",
    "        self.softmax = th.nn.Softmax(dim=2)\n",
    "        self.sigmoid = th.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.encoder(x)\n",
    "        x1, _ = self.decoder1(x)\n",
    "        x2, _ = self.decoder2(x)\n",
    "        x1 = self.linear1(x1)\n",
    "        x1 = self.softmax(x1)\n",
    "        x2 = self.linear2(x2)\n",
    "        x2 = self.sigmoid(x2)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset preparation\n",
    "Use the ReviewDataset to create a DataLoader\n",
    "Splitting the train, validation, and test sets\n",
    "\"\"\"\n",
    "# initialise the dataset\n",
    "dataset = ReviewDataset(df)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "# shrink dataset for testing\n",
    "dataset_size = 500\n",
    "dataset = th.utils.data.Subset(dataset, range(dataset_size))\n",
    "\n",
    "# split the dataset\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# create the dataloaders\n",
    "batch_size = 32\n",
    "train_loader = th.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = th.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 18152]) torch.Size([32]) torch.Size([32, 151])\n"
     ]
    }
   ],
   "source": [
    "# test the dataloader\n",
    "train_loader_iter = iter(train_loader)\n",
    "x, y, z = next(train_loader_iter)\n",
    "print(x.shape, y.shape, z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "\"\"\" # initialise the model\n",
    "# take into account if it is a subset of the dataset\n",
    "model = Summariser(dataset.dataset.vocab_size, 256, max_review_len, max_summary_len)\n",
    "model = model.to(device)\n",
    "\n",
    "# define the loss functions\n",
    "loss_fn1 = th.nn.CrossEntropyLoss()\n",
    "loss_fn2 = th.nn.BCELoss()\n",
    "\n",
    "# define the optimiser\n",
    "optimiser = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# train the model\n",
    "for epoch in range(epochs):\n",
    "    for review, rating, summary in train_loader:\n",
    "        # zero the gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        y_pred1, y_pred2 = model(review)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss1 = loss_fn1(y_pred1, summary)\n",
    "        loss2 = loss_fn2(y_pred2, rating.unsqueeze(1).float())\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimiser.step()\n",
    "\n",
    "    # print the loss\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}') \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
