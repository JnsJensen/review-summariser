{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\Kevork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 2080\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\Kevork\\AppData\\Local\\Temp\\ipykernel_18056\\3174333581.py:24: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
                  "  plt.style.use(\"seaborn\")\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, GPT2TokenizerFast\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "import os\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# use seaborn style for matplotlib\n",
            "plt.style.use(\"seaborn\")\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "# device = th.device(\"cpu\")\n",
            "\n",
            "fig_dir = \"img/\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' # Use simple GPT2 tokenizer for counting tokens\\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\nprint(f\"Original len: {len(df)}\")\\ndf_pruned = util.prune(df)\\nprint(f\"Pruned len: {len(df_pruned)}\")\\nutil.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\\n\\ndf_tokenized = util.tokenize(df_pruned, tokenizer)\\nprint(f\"Tokenized len: {len(df_tokenized)}\")\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df_tokenized[\\'reviewText\\'].arr.lengths()))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df_tokenized[\\'summary\\'].arr.lengths())))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\" # Use simple GPT2 tokenizer for counting tokens\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "print(f\"Original len: {len(df)}\")\n",
            "df_pruned = util.prune(df)\n",
            "print(f\"Pruned len: {len(df_pruned)}\")\n",
            "util.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "\n",
            "df_tokenized = util.tokenize(df_pruned, tokenizer)\n",
            "print(f\"Tokenized len: {len(df_tokenized)}\")\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df_tokenized['reviewText'].arr.lengths()))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df_tokenized['summary'].arr.lengths())))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, length = None, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        if length is not None:\n",
            "            # clip the dataset to length\n",
            "            length = min(length, len(self.df))\n",
            "            self.df = self.df.sample(length, shuffle=True)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=False, trim_offsets=True)\\nt.pad_token = t.eos_token\\nt.add_special_tokens({\"bos_token\": util.BOS_token})\\n\\n# Create the dataset\\ndataset = ReviewDataset(util.Paths.arts, t, device=device)\\n\\ndata_idx = 45\\n\\n# decode\\nprint(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\\nprint(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\\nprint(f\"Rating: {int(dataset[data_idx][2])}\")\\n\\n# max length is the max index of the vocabulary\\nMAX_LENGTH = len(t)\\nprint(f\"MAX_LENGTH: {MAX_LENGTH}\") '"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "\"\"\" t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=False, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.bidirectional = bidirectional\n",
            "        self.num_layers = num_layers\n",
            "        self.hidden_size = hidden_size\n",
            "        self.embedding = nn.Embedding(input_size, self.hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
            "        # self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=False)\n",
            "\n",
            "    # def forward(self, input, hidden):\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        context_vector, hidden = self.gru(embedded, hidden)\n",
            "        # context_vector, (hidden, cell_state) = self.lstm(embedded, (hidden, th.zeros_like(hidden, device=device)))\n",
            "        return context_vector, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        dimension_1 = self.num_layers * (2 if self.bidirectional else 1)\n",
            "        return th.zeros(dimension_1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, max_length, num_layers=1, bidirectional=True):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.bidirectional = bidirectional\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.max_length = max_length # The size of the vocabulary - len(tokenizer)\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
            "        # self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=num_layers, bidirectional=False)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, context_vector):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), context_vector.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        # output, (hidden, cell_state) = self.lstm(output, (hidden, th.zeros_like(hidden, device=device)))\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        dimension_1 = self.num_layers * (2 if self.bidirectional else 1)\n",
            "        return th.zeros(dimension_1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# For saving the states of the model, optimiser and epoch nr.\n",
            "def save_state(loss, accuracy, encoder_model, decoder_model, encoder_optimiser, decoder_optimiser, iteration, run_dir):\n",
            "    print(f\"Saving model at iteration {iteration} with loss {loss} and accuracy {accuracy}\")\n",
            "    checkpoint = {\n",
            "        \"loss\": loss,\n",
            "        \"accuracy\": accuracy,\n",
            "        \"encoder_model_state\": encoder_model.state_dict(),\n",
            "        \"decoder_model_state\": decoder_model.state_dict(),\n",
            "        \"encoder_optimiser_state\": encoder_optimiser.state_dict(),\n",
            "        \"decoder_optimiser_state\": decoder_optimiser.state_dict(),\n",
            "        \"iteration\": iteration\n",
            "    }\n",
            "    for file in os.listdir(f'checkpoints/{run_dir[5:8]}'):\n",
            "        os.remove(f\"checkpoints/{run_dir[5:8]}/{file}\")\n",
            "    model_checkpoint_path = f\"checkpoints/{run_dir[5:8]}/{run_dir[9:]}_l_{loss}_a_{accuracy}_{iteration}.pth\"\n",
            "    th.save(checkpoint, model_checkpoint_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Current run_dir: runs/084_mv_2.2.0_GRU_bi_nl_2_bs_64_lr_0.0005_tfr_0.5_hs_256_ds_8000\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Prepare for training\n",
            "debugging = True # For debugging prints\n",
            "model_version = \"2.2.0_GRU_bi\"\n",
            "n_epochs = 10\n",
            "batch_size = 64\n",
            "learning_rate = 0.0005\n",
            "teacher_forcing_ratio = 0.5\n",
            "hidden_size = 2**8 # 256\n",
            "dataset_size = 8000\n",
            "num_layers = 2 # LSTM or GRU layers\n",
            "bidirectional = False\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "run_dir = util.get_run_dir()\n",
            "# add run info to the directory\n",
            "run_dir += f\"_mv_{model_version}_nl_{num_layers}_bs_{batch_size}_lr_{learning_rate}_tfr_{teacher_forcing_ratio}_hs_{hidden_size}_ds_{dataset_size}\"\n",
            "print(f\"Current run_dir: {run_dir}\")\n",
            "# Readying the writer\n",
            "writer = SummaryWriter(run_dir)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# criterion = nn.CrossEntropyLoss(label_smoothing=0.1) # TODO: Check without the ignore_index\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "# Instantiate tokenizer\n",
            "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "tokenizer.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(tokenizer)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, num_layers=num_layers, max_length=MAX_LENGTH, bidirectional=bidirectional).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, tokenizer, length=dataset_size, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "\n",
            "# Get the data loaders\n",
            "train_loader, val_loader, test_loader = util.get_data_loaders(dataset, batch_size, split_ratios)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "def val(val_iter, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    with th.no_grad():\n",
            "        encoder.eval()\n",
            "        decoder.eval()\n",
            "        # We calcualte the loss and backpropagate every batch\n",
            "        # Reset the gradients\n",
            "        encoder_optimizer.zero_grad()\n",
            "        decoder_optimizer.zero_grad()\n",
            "\n",
            "        batch_loss = 0\n",
            "        words_in_batch = 0\n",
            "        correct_words_in_batch = 0\n",
            "\n",
            "        val_review_batch, val_summary_batch, val_rating_batch = next(val_iter)\n",
            "        for val_review, val_summary, val_rating in zip(val_review_batch, val_summary_batch, val_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Get the length of the review\n",
            "            review_length = val_review.shape[0]\n",
            "            summary_length = val_summary.shape[0]\n",
            "\n",
            "            # Create the encoder outputs tensor\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size * (2 if encoder.bidirectional else 1), device=device)\n",
            "\n",
            "            # Encoder forward pass\n",
            "            for ei in range(review_length):\n",
            "                encoder_output, encoder_hidden = encoder(val_review[ei], encoder_hidden)\n",
            "                encoder_outputs[ei] += encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            bos = th.tensor(tokenizer.bos_token_id).to(device)\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Initialize the decoder output\n",
            "            decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.long).fill_(tokenizer.pad_token_id)\n",
            "            decoder_output_sequence[0] = decoder_input\n",
            "\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Decoder forward pass\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(val_summary[1:]):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index+1] = decoder_input\n",
            "\n",
            "                # Count the correct words\n",
            "                print(f\"Pred: {decoder_input.item()}, Target: {target.item()}\")\n",
            "                if decoder_input.item() == target.item():\n",
            "                    correct_words_in_batch += 1\n",
            "\n",
            "                words_in_batch += 1\n",
            "                # Calculate the loss\n",
            "                batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                    #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                    break\n",
            "\n",
            "        encoder.train()\n",
            "        decoder.train()\n",
            "\n",
            "        # Normalize the loss and accuracy\n",
            "        batch_loss /= words_in_batch\n",
            "        accuracy = correct_words_in_batch / words_in_batch\n",
            "\n",
            "        return accuracy, batch_loss.item(), decoder_output_sequence"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [],
         "source": [
            "def train(learning_rate, val_loader, n_epochs, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    min_loss = np.inf\n",
            "    for epoch in range(n_epochs):\n",
            "\n",
            "        val_iter = iter(val_loader)\n",
            "\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "            correct_words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                context_vector = th.zeros(MAX_LENGTH, encoder.hidden_size * (2 if encoder.bidirectional else 1), device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    context_vector[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(tokenizer.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.long).fill_(tokenizer.pad_token_id)\n",
            "                decoder_output_sequence[0] = decoder_input\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        #util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                        print(\"USING TEACHER FORCING\")\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary[1:]):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, context_vector)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        # Append the output\n",
            "                        prediction = topi.squeeze().detach() # detach from history as input\n",
            "                        decoder_output_sequence[target_index+1] = prediction\n",
            "\n",
            "                        # Count the correct words\n",
            "                        if prediction.item() == target.item():\n",
            "                            correct_words_in_batch += 1\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                            #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                            break\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        #util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.CYAN])\n",
            "                        print(\"NOT USING TEACHER FORCING\")\n",
            "\n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary[1:]):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, context_vector)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index+1] = decoder_input\n",
            "\n",
            "                        # Count the correct words\n",
            "                        if decoder_input.item() == target.item():\n",
            "                            correct_words_in_batch += 1\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == tokenizer.eos_token_id:\n",
            "                            #print(f\"EOS token found at iteration {target_index+1}\")\n",
            "                            break\n",
            "                \n",
            "                if debugging:\n",
            "                    # print tokenized output\n",
            "                    #util.print_mod(\"Target tokenized:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Target tokenized:\")\n",
            "                    print(summary.tolist())\n",
            "                    # util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(\"Target Sequence:\")\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    #util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Tokenized output:\")\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    #util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"Detokenized output:\")\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            \n",
            "            # Normalize the loss\n",
            "            batch_loss /= words_in_batch\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "            # Accuracy\n",
            "            accuracy = correct_words_in_batch / words_in_batch\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            iteration = epoch * len(train_loader) + batch_idx\n",
            "            # Print the loss and accuracy\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss, iteration)\n",
            "            writer.add_scalar(\"Accuracy/train\", accuracy, iteration)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}\")\n",
            "            # util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}\", [util.Modifiers.Colors.MAGENTA, util.Modifiers.Styles.BOLD])\n",
            "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss}, Accuracy: {accuracy}\")\n",
            "\n",
            "            # Validate the model\n",
            "            if batch_idx % 5 == 0:\n",
            "                val_acc, val_loss, val_sequence = val(val_iter, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
            "                val_sequence_detokenized = dataset.detokenize(val_sequence)\n",
            "                writer.add_scalar(\"Loss/val\", val_loss, iteration)\n",
            "                writer.add_scalar(\"Accuracy/val\", val_acc, iteration)\n",
            "                \n",
            "                if val_loss < min_loss and iteration > 125:\n",
            "                    save_state(val_loss, val_acc, encoder, decoder, encoder_optimizer, decoder_optimizer, val_loss, iteration, run_dir)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[845, 2495, 475, 407, 845, 23408, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " very pretty but not very sticky\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[857, 1693, 475, 1165, 3538, 30795, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " does job but too easily detached\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 1842, 777, 36116, 290, 1312, 716, 845, 3772, 351, 616, 5001, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i love these beads and i am very happy with my purchase\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[28175, 2562, 290, 1049, 329, 2010, 10046, 289, 15712, 275, 14146, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " relaxing easy and great for netflix hulu binging\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2266, 17118, 11073, 1194, 1049, 2891, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " red devil produces another great tool\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 1720, 373, 7151, 329, 257, 27623, 1304, 88, 17763, 1486, 351, 4457, 15715, 48945, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this product was recommended for a embroidery tile design with extremely dense stitching\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 20946, 23521, 23521, 23521, 23521, 29586, 46916, 439, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanksakespeareanguardanguardanguardanguard rumoredpletsallanguardanguardanguard\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[922, 3124, 611, 257, 1310, 7888, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " good color if a little thin\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 329, 477, 17548, 7912, 1262, 3348, 393, 7914, 7090, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great for all sketch artists using paper or electronic medium\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 697, 27547, 27547, 27547, 19281, 27547, 23428, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Don accthanksthanksthanks SchedulethanksVAL\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 2818, 1517, 284, 910, 284, 2687, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the perfect thing to say to anyone\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 27547, 27547, 16723, 27547, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remainingthanksthanks pondthanks\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[4724, 345, 761, 257, 1365, 44120, 329, 326, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " guess you need a better magnification for that\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 329, 262, 717, 640, 779, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great for the first time use\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 5637, 14909, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remaining remaining Madison Risk\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[857, 407, 804, 1997, 588, 262, 2656, 355, 64, 16082, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " does not look anything like the original asa patches\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 1956, 19602, 19602, 29586, 19281, 18076, 18076, 48190, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don land Risk Risk rumored Scheduleoptionoption photon\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 1043, 257, 835, 284, 779, 777, 287, 616, 11376, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i found a way to use these in my garden\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 27547, 27547, 27547, 22663, 27547, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanksthanksthanksthanks Supplythanksanguard\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[991, 2045, 329, 262, 2818, 2891, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " still looking for the perfect tool\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[922, 10230, 829, 466, 262, 1693, 16576, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " good buckles do the job nicely\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 9154, 3953, 15728, 329, 15964, 8331, 2626, 1141, 3463, 2994, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great tape measure handy for measuring inches lost during weight loss\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 41823, 27547, 697, 14909, 14909, 19602, 27547, 2290, 30951, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Donalosthanks acc Madison Madison Riskthanksluatalie\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[20239, 1049, 329, 300, 1092, 1371, 290, 584, 22634, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " colorful great for lanyards and other jewelry\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 20946, 27547, 23521, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remainingakespearethanksanguardanguardanguardanguard\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 262, 3124, 7466, 616, 1097, 18647, 15224, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love the color matches my carhart jacket\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 340, 290, 2562, 284, 779, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love it and easy to use\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 5637, 42948, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remaining remaininghaar Risk\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2832, 866, 262, 1266, 12967, 902, 1312, 423, 973, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " hands down the best pigments i have used\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 5637, 5637, 23521, 23521, 23521, 19602, 20946, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don remaining remaininganguardanguardanguard Riskakespeare\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2266, 2612, 2208, 473, 332, 474, 29309, 21181, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " red heart super saver jumbo yarn\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 5637, 14909, 14909, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remaining remaining Madison Madisonanguardanguard\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[4833, 621, 262, 4286, 1838, 340, 804, 475, 1312, 1842, 340, 477, 262, 976, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " smaller than the picture makes it look but i love it all the same\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2811, 20735, 355, 922, 355, 644, 2058, 319, 8632, 422, 262, 8860, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " average thickness as good as what comes on seats from the factory\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[777, 26569, 257, 1256, 284, 1394, 28096, 772, 290, 1842, 262, 886, 2482, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " these hooks a lot to keep stitches even and love the end results\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 262, 835, 484, 389, 25555, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love the way they are packaged\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 8837, 2262, 2262, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remaining vessel Inst Inst\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[6938, 7932, 1312, 3589, 1842, 773, 666, 2057, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " taste wonderful i myself love indian food\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 15236, 15236, 21123, 42699, 42699, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donconnectedconnected reim facilitates facilitates\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 1720, 373, 2818, 3446, 644, 1312, 2622, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the product was perfect exactly what i needed\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[257, 1310, 45553, 481, 466, 21349, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " a little dab will do ya\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 27547, 27547, 30526, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remainingthanksthanksmarried\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[11721, 290, 881, 1365, 621, 262, 5449, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " cheaper and much better than the competition\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 5637, 27547, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining remainingthanksanguardanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 1310, 9154, 3953, 2499, 1049, 2818, 329, 10627, 534, 2546, 618, 16216, 9528, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this little tape measure works great perfect for checking your size when ordering clothing\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 27547, 19281, 19281, 22800, 22800, 19281, 23521, 23521, 5376, 5376, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donthanks Schedule Schedule252252 ScheduleanguardanguardNameName Risk\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[8390, 407, 3869, 458, 515, 379, 477, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " fake not gold plated at all\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[703, 750, 1312, 1683, 627, 2326, 1231, 777, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " how did i ever quilt without these\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 27547, 10596, 18076, 19281, 19281, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donthanks Atlanticoption Schedule Schedule\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[777, 389, 845, 3621, 475, 466, 407, 2230, 284, 42809, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " these are very nice but do not attempt to solder\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[3446, 644, 1312, 2938, 379, 257, 1049, 1730, 7720, 777, 318, 257, 1310, 2408, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " exactly what i expected at a great deal cutting these is a little difficult\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[655, 644, 356, 547, 2045, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " just what we were looking for\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1266, 307, 4980, 25209, 1312, 423, 1683, 973, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " best beading needles i have ever used\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 27547, 27547, 27547, 27547, 20946, 20946, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Donthanksthanksthanksthanksakespeareakespeare\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[9154, 2499, 1049, 665, 33105, 364, 389, 2087, 5556, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " tape works great tweezers are added plus\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 41823, 41823, 14909, 1251, 29022, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingalosalos Madisonchool509anguardanguard\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[262, 3124, 373, 4950, 290, 262, 21181, 373, 845, 2705, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " the color was beautiful and the yarn was very soft\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[638, 1967, 264, 11293, 389, 262, 1266, 25209, 503, 612, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " knitter s pride are the best needles out there\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[373, 407, 1498, 284, 651, 340, 284, 670, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " was not able to get it to work\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 2094, 27547, 27547, 50009, 16723, 22663, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don Donthanksthanks strutConnector pond Supply\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1842, 340, 329, 281, 4072, 793, 278, 2603, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " love it for an embossing mat\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[466, 407, 7030, 534, 1637, 319, 428, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " do not waste your money on this\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 318, 5543, 4950, 290, 612, 318, 523, 34160, 881, 286, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this is absolutely beautiful and there is soooo much of it\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 20946, 23521, 23521, 23521, 23521, 23521, 19281, 19281, 22663, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donakespeareanguardanguardanguardanguardanguard Schedule Schedule Supply\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[428, 318, 257, 1049, 16001, 1660, 8043, 900, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " this is a great compact watercolor set\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 23521, 23521, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madisonanguardanguard Risk\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[374, 2509, 502, 428, 644, 466, 39694, 290, 19553, 423, 287, 2219, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " riddle me this what do ducks and bats have in common\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 27547, 29586, 23521, 23521, 23521, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanksthanks rumoredanguardanguardanguardanguardanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 14093, 1312, 588, 262, 1588, 530, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great brush i like the large one\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 27547, 27547, 27547, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donthanksthanksthanks Risk\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[3595, 3081, 714, 423, 1760, 428, 572, 286, 23645, 4263, 3589, 329, 1479, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " poor quality could have done this off of google images myself for free\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 697, 20946, 20946, 19281, 20946, 19281, 19281, 19281, 19281, 19281, 29022, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining accakespeareakespeare Scheduleakespeare Schedule Schedule Schedule Schedule Schedule509\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1312, 1842, 2279, 546, 428, 35356, 4572, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " i love everything about this sewing machine\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2042, 285, 404, 318, 407, 4961, 284, 2330, 285, 404, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " black mop is not equal to white mop\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[655, 644, 1312, 373, 2045, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " just what i was looking for\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2975, 2094, 27547, 19281, 19281, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining road Donthanks Schedule Schedule\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[2562, 284, 779, 290, 14169, 1486, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " easy to use and clever design\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 27547, 14909, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanksthanks Madison\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[922, 41786, 4601, 340, 373, 517, 10935, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " good scissors wish it was more affordable\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[304, 74, 1943, 279, 2777, 24142, 3980, 1588, 3072, 5228, 629, 439, 404, 10862, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " ek success pspwp56 large phone corner scallop punch\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27417, 27547, 19281, 19281, 19281, 27547, 19602, 19602, 19602, 27547, 2290, 33766, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining calibthanks Schedule Schedule Schedulethanks Risk Risk Riskthanksluaraoh\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1838, 28967, 21181, 523, 2562, 290, 1611, 286, 1257, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " makes winding yarn so easy and kind of fun\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 6292, 6292, 19281, 23521, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don accepted accepted Scheduleanguardanguardanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[18344, 572, 7521, 36209, 691, 2063, 1336, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " rip off paint pans only half full\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 20946, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanksakespeareanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[484, 547, 881, 4833, 621, 1312, 2938, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " they were much smaller than i expected\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 43971, 27547, 23428, 23428, 23428, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don BerserkerthanksVALVALVAL\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[4957, 10408, 428, 290, 607, 289, 84, 295, 290, 3544, 606, 2048, 4445, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " daughter loves this and her huion and uses them almost daily\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 2094, 41823, 19281, 23521, 23521, 19281, 23521, 23521, 23521, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don Donalos Scheduleanguardanguard Scheduleanguardanguardanguardanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[922, 1720, 922, 2756, 2068, 7585, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " good product good price quick delivery\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 41823, 27547, 23521, 23521, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingalosthanksanguardanguard\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[1049, 1752, 1312, 11638, 503, 703, 284, 1280, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " great once i figured out how to open it\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 2094, 27547, 27547, 14909, 25389, 19281, 22663, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remaining Donthanksthanks Madison Trevor Schedule Supply\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[407, 3772, 351, 262, 2546, 286, 262, 9294, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " not happy with the size of the bottle\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[4171, 2603, 318, 523, 2861, 340, 1049, 18537, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " blue mat is so worth it great bundle\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 2094, 20946, 19281, 19281, 19602, 29022, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don Donakespeare Schedule Schedule Risk509\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[880, 925, 1720, 290, 2562, 284, 4174, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " well made product and easy to apply\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 2094, 2094, 23521, 23521, 42948, 19602, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining Don Donanguardanguardhaar Risk\n",
                  "\n",
                  "USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[340, 2499, 3734, 475, 262, 7585, 373, 8812, 540, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " it works fine but the delivery was terrable\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 41823, 41823, 23521, 23521, 23521, 19281, 19281, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Detokenized output:\n",
                  " remaining remainingalosalosanguardanguardanguard Schedule Schedule\n",
                  "\n",
                  "NOT USING TEACHER FORCING\n",
                  "Target tokenized:\n",
                  "[3612, 1312, 815, 2822, 1194, 379, 428, 2756, 4313, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "Target Sequence:\n",
                  " thinking i should buy another at this price recommend\n",
                  "Tokenized output:\n",
                  "[50257, 5637, 5637, 27547, 27547, 14909, 14909, 14909, 14909, 1251, 42699, 42699, 42699, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 22663, 37066, 19281, 19281, 14909, 14909, 14909, 14909, 1326, 1326, 14743, 14743, 27547, 14909, 14909, 1326, 14743]\n",
                  "Detokenized output:\n",
                  " remaining remainingthanksthanks Madison Madison Madison Madisonchool facilitates facilitates facilitates Supply Supply Supply Supply Supply Supply Supply Supply totaling Schedule Schedule Madison Madison Madison Madisonmeme Richards Richardsthanks Madison Madisonme Richards\n",
                  "\n",
                  "Epoch: 0, Batch: 0, Loss: 10.741559982299805, Accuracy: 0.0\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 407, Target: 257\n",
                  "Pred: 20946, Target: 845\n",
                  "Pred: 50256, Target: 1402\n",
                  "Pred: 407, Target: 4833\n",
                  "Pred: 407, Target: 621\n",
                  "Pred: 20946, Target: 2938\n",
                  "Pred: 50256, Target: 290\n",
                  "Pred: 407, Target: 2837\n",
                  "Pred: 407, Target: 286\n",
                  "Pred: 20946, Target: 36377\n",
                  "Pred: 50256, Target: 329\n",
                  "Pred: 407, Target: 389\n",
                  "Pred: 407, Target: 4950\n",
                  "Pred: 20946, Target: 475\n",
                  "Pred: 50256, Target: 599\n",
                  "Pred: 407, Target: 621\n",
                  "Pred: 407, Target: 2938\n",
                  "Pred: 20946, Target: 351\n",
                  "Pred: 50256, Target: 257\n",
                  "Pred: 407, Target: 318\n",
                  "Pred: 407, Target: 845\n",
                  "Pred: 20946, Target: 922\n",
                  "Pred: 50256, Target: 3703\n",
                  "Pred: 407, Target: 6630\n",
                  "Pred: 407, Target: 3348\n",
                  "Pred: 20946, Target: 407\n",
                  "Pred: 50256, Target: 4190\n",
                  "Pred: 407, Target: 43657\n",
                  "Pred: 407, Target: 537\n",
                  "Pred: 20946, Target: 292\n",
                  "Pred: 50256, Target: 862\n",
                  "Pred: 407, Target: 318\n",
                  "Pred: 407, Target: 257\n",
                  "Pred: 20946, Target: 922\n",
                  "Pred: 50256, Target: 4096\n",
                  "Pred: 407, Target: 407\n",
                  "Pred: 407, Target: 4988\n",
                  "Pred: 20946, Target: 1745\n",
                  "Pred: 50256, Target: 7982\n",
                  "Pred: 407, Target: 804\n",
                  "Pred: 407, Target: 922\n",
                  "Pred: 20946, Target: 290\n",
                  "Pred: 50256, Target: 389\n",
                  "Pred: 407, Target: 4047\n",
                  "Pred: 407, Target: 5220\n",
                  "Pred: 20946, Target: 329\n",
                  "Pred: 50256, Target: 6546\n",
                  "Pred: 407, Target: 1969\n",
                  "Pred: 407, Target: 905\n",
                  "Pred: 20946, Target: 64\n",
                  "Pred: 50256, Target: 479\n",
                  "Pred: 407, Target: 716\n",
                  "Pred: 407, Target: 845\n",
                  "Pred: 20946, Target: 11378\n",
                  "Pred: 50256, Target: 351\n",
                  "Pred: 407, Target: 407\n",
                  "Pred: 407, Target: 670\n",
                  "Pred: 20946, Target: 319\n",
                  "Pred: 50256, Target: 3956\n",
                  "Pred: 407, Target: 41786\n",
                  "Pred: 407, Target: 389\n",
                  "Pred: 20946, Target: 4998\n",
                  "Pred: 50256, Target: 484\n",
                  "Pred: 407, Target: 2331\n",
                  "Pred: 407, Target: 588\n",
                  "Pred: 20946, Target: 257\n",
                  "Pred: 50256, Target: 1218\n",
                  "Pred: 407, Target: 373\n",
                  "Pred: 407, Target: 257\n",
                  "Pred: 20946, Target: 1049\n",
                  "Pred: 50256, Target: 1720\n",
                  "Pred: 407, Target: 318\n",
                  "Pred: 407, Target: 2818\n",
                  "Pred: 20946, Target: 900\n",
                  "Pred: 50256, Target: 286\n",
                  "Pred: 407, Target: 1049\n",
                  "Pred: 407, Target: 900\n",
                  "Pred: 20946, Target: 286\n",
                  "Pred: 50256, Target: 1226\n",
                  "Pred: 407, Target: 588\n",
                  "Pred: 407, Target: 257\n",
                  "Pred: 20946, Target: 3024\n",
                  "Pred: 50256, Target: 9845\n",
                  "Pred: 407, Target: 716\n",
                  "Pred: 407, Target: 3221\n",
                  "Pred: 20946, Target: 845\n",
                  "Pred: 50256, Target: 11378\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 407, Target: 23481\n",
                  "Pred: 20946, Target: 1049\n",
                  "Pred: 50256, Target: 329\n",
                  "Pred: 407, Target: 7720\n",
                  "Pred: 407, Target: 24438\n",
                  "Pred: 20946, Target: 290\n",
                  "Pred: 50256, Target: 13621\n",
                  "Pred: 407, Target: 41786\n",
                  "Pred: 407, Target: 1312\n",
                  "Pred: 20946, Target: 716\n",
                  "Pred: 50256, Target: 9675\n",
                  "Pred: 407, Target: 655\n",
                  "Pred: 407, Target: 262\n",
                  "Pred: 20946, Target: 826\n",
                  "Pred: 50256, Target: 2033\n",
                  "Pred: 407, Target: 1913\n",
                  "Pred: 407, Target: 43608\n",
                  "Pred: 20946, Target: 2089\n",
                  "Pred: 50256, Target: 3124\n",
                  "Pred: 407, Target: 1718\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 20946, Target: 1683\n",
                  "Pred: 50256, Target: 475\n",
                  "Pred: 407, Target: 3081\n",
                  "Pred: 407, Target: 47750\n",
                  "Pred: 20946, Target: 29092\n",
                  "Pred: 50256, Target: 319\n",
                  "Pred: 407, Target: 46069\n",
                  "Pred: 407, Target: 351\n",
                  "Pred: 20946, Target: 257\n",
                  "Pred: 50256, Target: 3621\n",
                  "Pred: 407, Target: 281\n",
                  "Pred: 407, Target: 1772\n",
                  "Pred: 20946, Target: 1312\n",
                  "Pred: 50256, Target: 7620\n",
                  "Pred: 407, Target: 2060\n",
                  "Pred: 407, Target: 530\n",
                  "Pred: 20946, Target: 3111\n",
                  "Pred: 50256, Target: 1049\n",
                  "Pred: 407, Target: 3621\n",
                  "Pred: 407, Target: 475\n",
                  "Pred: 20946, Target: 635\n",
                  "Pred: 50256, Target: 845\n",
                  "Pred: 407, Target: 257\n",
                  "Pred: 407, Target: 1227\n",
                  "Pred: 20946, Target: 290\n",
                  "Pred: 50256, Target: 991\n",
                  "Pred: 407, Target: 4899\n",
                  "Pred: 407, Target: 389\n",
                  "Pred: 20946, Target: 20422\n",
                  "Pred: 50256, Target: 290\n",
                  "Pred: 407, Target: 20784\n",
                  "Pred: 407, Target: 389\n",
                  "Pred: 20946, Target: 1049\n",
                  "Pred: 50256, Target: 290\n",
                  "Pred: 407, Target: 86\n",
                  "Pred: 407, Target: 1045\n",
                  "Pred: 20946, Target: 27673\n",
                  "Pred: 50256, Target: 275\n",
                  "Pred: 407, Target: 407\n",
                  "Pred: 407, Target: 905\n",
                  "Pred: 20946, Target: 510\n",
                  "Pred: 50256, Target: 880\n",
                  "Pred: 407, Target: 1302\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 20946, Target: 534\n",
                  "Pred: 50256, Target: 9927\n",
                  "Pred: 407, Target: 373\n",
                  "Pred: 407, Target: 3538\n",
                  "Pred: 20946, Target: 5969\n",
                  "Pred: 50256, Target: 351\n",
                  "Pred: 407, Target: 1502\n",
                  "Pred: 407, Target: 428\n",
                  "Pred: 20946, Target: 1720\n",
                  "Pred: 50256, Target: 329\n",
                  "Pred: 407, Target: 1720\n",
                  "Pred: 407, Target: 2499\n",
                  "Pred: 20946, Target: 880\n",
                  "Pred: 50256, Target: 351\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 407, Target: 262\n",
                  "Pred: 20946, Target: 4295\n",
                  "Pred: 50256, Target: 2704\n",
                  "Pred: 407, Target: 1842\n",
                  "Pred: 407, Target: 477\n",
                  "Pred: 20946, Target: 953\n",
                  "Pred: 50256, Target: 279\n",
                  "Pred: 407, Target: 2435\n",
                  "Pred: 407, Target: 21181\n",
                  "Pred: 20946, Target: 6143\n",
                  "Pred: 50256, Target: 9396\n",
                  "Pred: 407, Target: 1842\n",
                  "Pred: 407, Target: 428\n",
                  "Pred: 20946, Target: 2378\n",
                  "Pred: 50256, Target: 1312\n",
                  "Pred: 407, Target: 17977\n",
                  "Pred: 407, Target: 318\n",
                  "Pred: 20946, Target: 2818\n",
                  "Pred: 50256, Target: 329\n",
                  "Pred: 407, Target: 3056\n",
                  "Pred: 407, Target: 475\n",
                  "Pred: 20946, Target: 9294\n",
                  "Pred: 50256, Target: 318\n",
                  "Pred: 407, Target: 3131\n",
                  "Pred: 407, Target: 1257\n",
                  "Pred: 20946, Target: 1312\n",
                  "Pred: 50256, Target: 1234\n",
                  "Pred: 407, Target: 750\n",
                  "Pred: 407, Target: 407\n",
                  "Pred: 20946, Target: 760\n",
                  "Pred: 50256, Target: 484\n",
                  "Pred: 407, Target: 670\n",
                  "Pred: 407, Target: 523\n",
                  "Pred: 20946, Target: 881\n",
                  "Pred: 50256, Target: 1365\n",
                  "Pred: 407, Target: 318\n",
                  "Pred: 407, Target: 523\n",
                  "Pred: 20946, Target: 2861\n",
                  "Pred: 50256, Target: 790\n",
                  "Pred: 407, Target: 7932\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 20946, Target: 19732\n",
                  "Pred: 50256, Target: 3996\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 407, Target: 8465\n",
                  "Pred: 20946, Target: 22634\n",
                  "Pred: 50256, Target: 645\n",
                  "Pred: 407, Target: 11499\n",
                  "Pred: 407, Target: 304\n",
                  "Pred: 20946, Target: 48524\n",
                  "Pred: 50256, Target: 6957\n",
                  "Pred: 407, Target: 3081\n",
                  "Pred: 407, Target: 329\n",
                  "Pred: 20946, Target: 257\n",
                  "Pred: 50256, Target: 1049\n",
                  "Pred: 407, Target: 14217\n",
                  "Pred: 407, Target: 6220\n",
                  "Pred: 20946, Target: 845\n",
                  "Pred: 50256, Target: 7613\n",
                  "Pred: 407, Target: 772\n",
                  "Pred: 407, Target: 3111\n",
                  "Pred: 20946, Target: 287\n",
                  "Pred: 50256, Target: 262\n",
                  "Pred: 407, Target: 477\n",
                  "Pred: 407, Target: 286\n",
                  "Pred: 20946, Target: 616\n",
                  "Pred: 50256, Target: 7521\n",
                  "Pred: 407, Target: 373\n",
                  "Pred: 407, Target: 407\n",
                  "Pred: 20946, Target: 523\n",
                  "Pred: 50256, Target: 1654\n",
                  "Pred: 407, Target: 611\n",
                  "Pred: 407, Target: 345\n",
                  "Pred: 20946, Target: 691\n",
                  "Pred: 50256, Target: 765\n",
                  "Pred: 407, Target: 389\n",
                  "Pred: 407, Target: 1049\n",
                  "Pred: 20946, Target: 38674\n",
                  "Pred: 50256, Target: 1312\n",
                  "Pred: 407, Target: 1730\n",
                  "Pred: 407, Target: 290\n",
                  "Pred: 20946, Target: 2562\n",
                  "Pred: 50256, Target: 284\n",
                  "Pred: 407, Target: 423\n",
                  "Pred: 407, Target: 925\n",
                  "Pred: 20946, Target: 5179\n",
                  "Pred: 50256, Target: 286\n"
               ]
            },
            {
               "ename": "TypeError",
               "evalue": "save_state() takes 8 positional arguments but 9 were given",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                  "\u001b[1;32m/home/dl21e22/review-summariser/summariser.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnode5/home/dl21e22/review-summariser/summariser.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(learning_rate, val_loader, n_epochs, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
                  "\u001b[1;32m/home/dl21e22/review-summariser/summariser.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate, val_loader, n_epochs, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnode5/home/dl21e22/review-summariser/summariser.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mAccuracy/val\u001b[39m\u001b[39m\"\u001b[39m, val_acc, iteration)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bnode5/home/dl21e22/review-summariser/summariser.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mif\u001b[39;00m val_loss \u001b[39m<\u001b[39m min_loss:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bnode5/home/dl21e22/review-summariser/summariser.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     save_state(val_loss, val_acc, encoder, decoder, encoder_optimizer, decoder_optimizer, val_loss, iteration, run_dir)\n",
                  "\u001b[0;31mTypeError\u001b[0m: save_state() takes 8 positional arguments but 9 were given"
               ]
            }
         ],
         "source": [
            "train(learning_rate, val_loader, n_epochs, train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "3947390f1d33bc3faf6525b63e52b94589ca7e97a4d338993fef3014bbdcaff9"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
