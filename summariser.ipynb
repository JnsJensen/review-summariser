{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 2070 SUPER\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "import os\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# use seaborn style for matplotlib\n",
            "plt.style.use(\"seaborn\")\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "# device = th.device(\"cpu\")\n",
            "\n",
            "fig_dir = \"img/\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' # Use simple GPT2 tokenizer for counting tokens\\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\nprint(f\"Original len: {len(df)}\")\\ndf_pruned = util.prune(df)\\nprint(f\"Pruned len: {len(df_pruned)}\")\\nutil.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\\n\\n# plot the review and summary word count distribution\\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\\n\\ntext_word_count = df_pruned[\\'reviewText\\'].apply(lambda x: len(x.split()))\\nsummary_word_count = df_pruned[\\'summary\\'].apply(lambda x: len(x.split()))\\n\\nfor ax, data, title in zip(ax.flatten(), [text_word_count, summary_word_count], [\\'Review Text\\', \\'Summary\\']):\\n    ax.hist(data, bins=50)\\n    ax.set_title(title)\\n    ax.set_xlabel(\\'Number of words\\')\\n    ax.set_ylabel(\\'Number of occurrences\\')\\n\\nplt.show()\\n\\n# save figure as png and pdf\\nfig.savefig(fig_dir + \"review_summary_word_count_distribution.png\", dpi=300)\\nfig.savefig(fig_dir + \"review_summary_word_count_distribution.pdf\")\\n\\ndf_tokenized = util.tokenize(df_pruned, tokenizer)\\nprint(f\"Tokenized len: {len(df_tokenized)}\")\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df_tokenized[\\'reviewText\\'].arr.lengths()))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df_tokenized[\\'summary\\'].arr.lengths())))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "\"\"\" # Use simple GPT2 tokenizer for counting tokens\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "print(f\"Original len: {len(df)}\")\n",
            "df_pruned = util.prune(df)\n",
            "print(f\"Pruned len: {len(df_pruned)}\")\n",
            "util.save_dataset(df_pruned, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "\n",
            "# plot the review and summary word count distribution\n",
            "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
            "\n",
            "text_word_count = df_pruned['reviewText'].apply(lambda x: len(x.split()))\n",
            "summary_word_count = df_pruned['summary'].apply(lambda x: len(x.split()))\n",
            "\n",
            "for ax, data, title in zip(ax.flatten(), [text_word_count, summary_word_count], ['Review Text', 'Summary']):\n",
            "    ax.hist(data, bins=50)\n",
            "    ax.set_title(title)\n",
            "    ax.set_xlabel('Number of words')\n",
            "    ax.set_ylabel('Number of occurrences')\n",
            "\n",
            "plt.show()\n",
            "\n",
            "# save figure as png and pdf\n",
            "fig.savefig(fig_dir + \"review_summary_word_count_distribution.png\", dpi=300)\n",
            "fig.savefig(fig_dir + \"review_summary_word_count_distribution.pdf\")\n",
            "\n",
            "df_tokenized = util.tokenize(df_pruned, tokenizer)\n",
            "print(f\"Tokenized len: {len(df_tokenized)}\")\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df_tokenized['reviewText'].arr.lengths()))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df_tokenized['summary'].arr.lengths())))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, length = None, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        if length is not None:\n",
            "            # clip the dataset to length\n",
            "            length = min(length, len(self.df))\n",
            "            self.df = self.df[:length]\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # Move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        # # Remove everything after the first <eos> token\n",
            "        # # This is important due to the fact that that output token is initialised with zeros\n",
            "        # is_eos = (x == self.tokenizer.eos_token_id).long()\n",
            "        # if is_eos.any():\n",
            "        #     x = x[:is_eos.argmax().item()]\n",
            "\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 6,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Current run_dir: runs/003_batch-size_16_learning-rate_0.005_teacher-forcing-ratio_0.5_hidden-size_256\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 991, 1107, 3621, 284, 1394, 8389, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but still really nice to keep organized\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 867, 514, 1095, 329, 428, 2891, 845, 15728, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " many usages for this tool very handy\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 25470, 8171, 220, 220, 5445, 8171, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " versa tip   broken tip\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 530, 286, 616, 3988, 4004, 13201, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " one of my kids favorite gifts \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 477, 37246, 8396, 8529, 4255, 12106, 389, 7932, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " all gooseberry patch cookbooks are wonderful \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 466, 407, 1842, 340, 475, 220, 1312, 466, 407, 5465, 340, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i do not love it but  i do not hate it \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 655, 644, 1312, 373, 2045, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " just what i was looking for\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 407, 1107, 644, 1312, 373, 3612, 286, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " not really what i was thinking of   \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 1312, 991, 1842, 340, 220, 7138, 23162, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but i still love it  perfectly imperfect\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 1754, 3975, 318, 257, 1049, 2126, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " butler map is a great idea\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1402, 220, 13779, 220, 22441, 290, 2562, 284, 31738, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " small  cute  shiny and easy to peel \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 428, 318, 1194, 1492, 286, 7572, 1312, 423, 407, 1865, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is another book of patterns i have not yet    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 8364, 28356, 220, 1178, 17978, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great recipe keeper  few flaws\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 716, 1262, 428, 329, 336, 3974, 1359, 616, 31323, 290, 16695, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am using this for stippling my grips and magazines    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 922, 3081, 290, 1049, 2756, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good quality and great price   \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 24203.0, 37635.0, 5544.0, 19182.0, 15761.0, 23086.0, 14856.0, 14856.0, 780.0, 19182.0, 15761.0, 30189.0, 29332.0, 721.0, 28154.0, 47412.0, 38730.0, 27063.0, 5544.0, 30176.0, 9503.0, 36664.0, 30888.0, 30888.0, 19813.0, 11751.0, 9200.0, 37067.0, 14856.0, 14856.0, 34859.0, 14856.0, 5351.0, 19182.0, 29903.0, 24916.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young meltingfocus fourthArray 1945 prosecuted ul ul becauseArray 1945asonictouchec YORK BarronHK Finding fourthsensitivecomm composeninenine crushrer nic diagrams ul ul harms ul bonArray 197 conflicting\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 618, 262, 4326, 651, 9442, 1576, 284, 2005, 262, 7309, 220, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " when the unit getshot enough to cut the plastic     \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[35465.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "young\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 0, Loss: 10.887152671813965\u001b[0m\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3621, 2891, 220, 561, 423, 587, 3621, 284, 423, 7729, 3017, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice tool  would have been nice to have instructions included \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 530, 4286, 2861, 257, 7319, 2456, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " one picture worth a thousand words\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 922, 2756, 329, 922, 19828, 14093, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good price for good foam brush\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 5906, 284, 779, 780, 21098, 616, 3290, 12606, 262, 27223, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " unable to use because sadly my dog crossed the rainbow    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 4998, 220, 3446, 644, 1312, 2227, 340, 284, 307, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " amazing  exactly what i wanted it to be \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 423, 1760, 530, 1786, 430, 1075, 319, 15955, 290, 340, 3111, 1049, 220, 5875, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i have done one engraving on aluminum and it worked great  thank\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3621, 1660, 8043, 3348, 284, 2193, 351, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice watercolor paper to learn with \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 5874, 2891, 319, 13991, 287, 7577, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " visual tool on variations in colors\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7427, 1257, 290, 9856, 1165, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome fun and educational too  \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1842, 428, 1492, 220, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " love this book     \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7818, 220, 761, 284, 1441, 340, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " terrible  need to return it\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1011, 503, 477, 534, 8993, 290, 340, 481, 407, 2270, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " take out all your anger and it will not break\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 922, 1700, 1492, 329, 257, 649, 15552, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a good record book for a new pup\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 46149, 11443, 427, 707, 7278, 220, 24638, 10848, 220, 642, 17059, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " crochet prayer shawls  leisure arts  5135 \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 845, 3621, 290, 1029, 3081, 900, 286, 21613, 82, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very nice and high quality set of pencils\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 2818, 329, 262, 1048, 508, 10408, 284, 2614, 1096, 511, 14296, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " perfect for the person who loves to personalize their recipes \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 1, Loss: inf\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 262, 4898, 5412, 318, 6792, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " the wood handle is comfortable \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 517, 621, 1138, 616, 9027, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " more than met my expectations \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 922, 285, 36096, 290, 262, 1492, 318, 880, 925, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but good mazes and the book is well made\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 8181, 319, 284, 517, 286, 534, 15756, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " hang on to more of your dough\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 1312, 716, 407, 355, 10607, 351, 777, 355, 1312, 1807, 1312, 561, 307, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but i am not as pleased with these as i thought i would be\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 11679, 287, 466, 407, 1309, 502, 3272, 625, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " disappointed in do not let me cross over \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1263, 4336, 286, 1310, 256, 522, 3186, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " big fan of little tike products \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 362, 2353, 3769, 2250, 286, 14676, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " 2 pack provides hours of satisfaction\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 750, 407, 1282, 351, 8171, 220, 220, 220, 220, 220, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " did not come with tip         \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1029, 8849, 329, 836, 2616, 479, 5549, 264, 649, 1492, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " high marks for donna kato s new book\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 777, 389, 257, 3081, 19828, 14093, 220, 1312, 779, 691, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " these are a quality foam brush  i use only    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 279, 4337, 7862, 642, 44218, 8576, 46603, 50128, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " porter cable 5554 1000 assorted biscuits\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 4545, 3511, 22632, 35356, 220, 4545, 3511, 22632, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " teach yourself visually sewing  teach yourself visually \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1468, 3918, 12692, 3253, 75, 416, 336, 272, 1636, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " old style scratch awl by stanley\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 513, 287, 352, 3124, 2891, 8155, 832, 716, 5168, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " 3 in 1 color tool purchased through amazon\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 627, 346, 889, 318, 257, 20005, 1312, 2648, 351, 616, 3656, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " quilting is a hobby i share with my wife\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 2, Loss: inf\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 477, 3835, 422, 285, 3808, 220, 479, 5549, 389, 6275, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " all books from mrs  kato are excellent\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3621, 2126, 220, 3348, 3081, 743, 307, 281, 2071, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice idea  paper quality may be an issue \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 407, 2818, 220, 475, 1365, 621, 749, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " not perfect  but better than most \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 281, 3090, 284, 616, 3272, 24695, 5888, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " an addition to my cross stitch library\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 329, 262, 2646, 4014, 287, 534, 1641, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for the film critic in your family\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 884, 257, 1180, 220, 14528, 220, 319, 9664, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " such a different  twist  on fabric \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 6041, 286, 627, 346, 889, 7605, 4893, 290, 468, 3294, 2239, 416, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " lots of quilting techniques explained and has multiple step by    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 716, 281, 32450, 627, 346, 353, 290, 892, 428, 561, 307, 523, 881, 1257, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i am an avid quilter and think this would be so much fun \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 288, 2787, 417, 1838, 1049, 3186, 220, 290, 428, 3111, 355, 2938, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " dremel makes great products  and this worked as expected\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 655, 644, 1312, 6149, 220, 220, 5556, 257, 7202, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " just what i ordered   plus a bonus \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 8359, 416, 1367, 614, 1468, 2576, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " enjoyed by 11 year old girl\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 1276, 423, 329, 534, 4947, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a must have for your collection \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 922, 3081, 220, 220, 15728, 329, 1402, 1660, 8043, 4493, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " good quality   handy for small watercolor projects\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1492, 329, 3726, 35938, 427, 707, 75, 30495, 1010, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great book for beginning lace shawl knitters \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1643, 31210, 220, 475, 857, 262, 1693, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " bit noisy  but does the job\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 777, 19828, 36377, 389, 262, 1266, 1695, 6609, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " these foam brushes are the best available anywhere\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 3, Loss: inf\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 857, 407, 670, 329, 12724, 7521, 503, 286, 8242, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " does not work for cleaning paint out of clothes\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7026, 1720, 3688, 284, 281, 4697, 336, 272, 1636, 3253, 75, 286, 262, 976, 2546, 2099, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " cheap product compared to an older stanley awl of the same size type\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 329, 2614, 779, 220, 475, 407, 329, 4708, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for personal use  but not for professional\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 16555, 1312, 714, 423, 1043, 262, 3218, 8879, 8313, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " wished i could have found the regular ruled edition\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 36228, 5013, 416, 597, 584, 1438, 318, 257, 4836, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a biscuit by any other name is a roll\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 477, 523, 2495, 220, 340, 318, 1327, 284, 5409, 543, 39585, 284, 787, 717, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " all so pretty  it is hard to decide which ont to make first\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 845, 2495, 220, 1049, 329, 5456, 8292, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very pretty  great for client meetings \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 41577, 3450, 220, 220, 7325, 7572, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " knitting nature   creative patterns\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 428, 318, 2192, 1541, 616, 4004, 627, 2326, 4896, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is probably already my favorite quilt investment\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7709, 475, 2227, 517, 286, 1123, 3124, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " decent but wanted more of each color\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 329, 4898, 16090, 661, 508, 18746, 290, 5461, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great for woodworking people who stain and finish  \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 4213, 329, 326, 2041, 6146, 259, 286, 21181, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great ideas for that special skein of yarn \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 40600, 220, 407, 1509, 42737, 20545, 24770, 220, 220, 645, 8465, 458, 803, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " beware  not swarovski crystals   no silver plating\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1310, 42506, 3989, 220, 407, 655, 329, 27197, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great little wellness journal  not just for workouts \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 777, 15907, 389, 10883, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " these charts are magical   \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1988, 329, 7572, 1312, 481, 1682, 779, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great value for patterns i will actually use \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 4, Loss: inf\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1492, 9675, 1312, 6497, 340, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great book glad i picked it \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1310, 256, 7938, 6833, 3084, 290, 18791, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " little tikes classic table and chairs\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 2208, 13779, 220, 1312, 5839, 428, 329, 616, 642, 614, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " super cute  i bought this for my 5 year    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1720, 220, 779, 340, 790, 1110, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great product  use it every day\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 523, 3772, 1312, 8155, 428, 1492, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " so happy i purchased this book \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7427, 18583, 220, 356, 2622, 428, 329, 1165, 890, 878, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome seller  we needed this for too long before    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 7932, 2691, 2055, 284, 1061, 355, 880, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a wonderful online community to follow as well\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 4724, 345, 1107, 651, 644, 345, 1414, 329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i guess you really get what you pay for\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 428, 318, 2818, 329, 2687, 351, 257, 1310, 2576, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " this is perfect for anyone with a little girl\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 340, 857, 262, 1693, 220, 1312, 716, 12617, 703, 340, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it does the job  i am impressed how it    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3111, 3734, 220, 788, 750, 407, 4894, 510, 845, 880, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " worked fine  then did not heat up very well\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1838, 257, 1256, 286, 9664, 7030, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " makes a lot of fabric waste\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1720, 220, 1049, 18371, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great product  great vendor \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1194, 12291, 319, 262, 6833, 326, 318, 1682, 2499, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " another variation on the classic that is actually works\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 36464, 3084, 290, 5118, 900, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a timeless table and chair set \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 329, 8296, 262, 1593, 1243, 287, 1204, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " for recording the important things in life \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 5, Loss: inf\u001b[0m\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 3253, 75, 220, 290, 2058, 351, 257, 2495, 880, 5447, 220, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great awl  and comes with a pretty well defined    \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 543, 1312, 588, 517, 220, 3073, 4735, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " which i like more  looks solid\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 845, 2562, 284, 779, 475, 530, 13502, 290, 663, 3750, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very easy to use but one wash and its gone\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 6979, 329, 534, 11077, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great gift for your girlfriend \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 15728, 329, 257, 649, 627, 346, 353, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " handy for a new quilter \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 588, 340, 475, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " like it but   \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 845, 1611, 290, 262, 1266, 18583, 220, 845, 880, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " very kind and the best seller  very well\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3621, 7572, 220, 3621, 2756, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice patterns  nice price \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 33975, 638, 896, 1165, 881, 670, 329, 6380, 1988, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " unnatural knits too much work for shock value\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 588, 477, 262, 2239, 416, 2239, 3124, 27329, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i like all the step by step color visuals\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 6275, 1720, 220, 475, 407, 30182, 13288, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " excellent product  but not bleedproof\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 4193, 502, 287, 257, 25394, 220, 4047, 4313, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " helped me in a pinch  highly recommend  \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1256, 264, 286, 627, 2326, 12779, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " lot s of quilt possibilities \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 836, 2616, 264, 7729, 389, 523, 2562, 284, 1833, 290, 1061, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " donna s instructions are so easy to understand and follow\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 7427, 220, 7141, 6764, 220, 220, 19376, 21898, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " awesome  precise description   timely mailing \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 475, 584, 621, 326, 340, 373, 1049, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " but other than that it was great \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[94m\u001b[1mEpoch: 0, Batch: 6, Loss: inf\u001b[0m\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 340, 318, 783, 257, 922, 10345, 3253, 75, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " it is now a good functional awl\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 1786, 430, 332, 257, 1276, 423, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great engraver a must have \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3988, 481, 1842, 340, 220, 468, 2279, 345, 761, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " kids will love it  has everything you need \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 2818, 9473, 329, 867, 1180, 5479, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " perfect papers for many different applications \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1049, 2891, 220, 973, 340, 284, 1786, 5758, 6147, 698, 2283, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " great tool  used it to engrave metal blanks\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1111, 389, 2495, 881, 262, 976, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " both are pretty much the same\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1266, 19828, 36377, 326, 1312, 423, 1683, 8155, 290, 973, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " best foam brushes that i have ever purchased and used\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 3621, 20922, 329, 534, 1266, 14296, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " nice notebook for your best recipes\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 257, 922, 923, 220, 475, 326, 318, 477, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " a good start  but that is all\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 4249, 993, 308, 1567, 77, 264, 1492, 318, 257, 670, 286, 1242, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " norah gaughn s book is a work of art\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 5543, 4998, 290, 2208, 2562, 284, 779, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " absolutely amazing and super easy to use\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 262, 3252, 318, 3750, 220, 220, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " the fear is gone   \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 750, 407, 670, 355, 880, 355, 1312, 2227, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " did not work as well as i wanted \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1310, 256, 7938, 3084, 290, 18791, 900, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " little tikes table and chairs set\n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[92mUSING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 1312, 550, 262, 17507, 326, 484, 561, 6437, 319, 262, 3348, 588, 9215, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " i had the expectation that they would rub on the paper like butter \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n",
                  "\u001b[91mNOT USING TEACHER FORCING\u001b[0m\n",
                  "EOS token found at iteration 0\n",
                  "\u001b[1m\u001b[3mTarget tokenized:\u001b[0m\n",
                  "[50256, 5802, 290, 5210, 3912, 276, 4417, 220, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]\n",
                  "\u001b[1m\u001b[3mTarget Sequence:\u001b[0m\n",
                  " tough and rough patterned surface \n",
                  "\u001b[1m\u001b[3mTokenized output:\u001b[0m\n",
                  "[50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0, 50256.0]\n",
                  "\u001b[1m\u001b[3mDetokenized output:\u001b[0m\n",
                  "\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "debugging = True # For debugging prints\n",
            "\n",
            "n_epochs = 10\n",
            "batch_size = 16\n",
            "learning_rate = 0.005\n",
            "teacher_forcing_ratio = 0.5\n",
            "hidden_size = 2**8\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "run_dir = util.get_run_dir()\n",
            "# add run info to the directory\n",
            "run_dir += f\"_batch-size_{batch_size}_learning-rate_{learning_rate}_teacher-forcing-ratio_{teacher_forcing_ratio}_hidden-size_{hidden_size}\"\n",
            "print(f\"Current run_dir: {run_dir}\")\n",
            "# Readying the writer\n",
            "writer = SummaryWriter(run_dir)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "# Instantiate tokenizer\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, tokenizer, length=800, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "\n",
            "# Get the data loaders\n",
            "train_loader, val_loader, test_loader = util.get_data_loaders(dataset, batch_size, split_ratios)\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.empty(dataset.max_summary_len, device=device, dtype=th.float).fill_(t.pad_token_id)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"USING TEACHER FORCING\", [util.Modifiers.Colors.GREEN])\n",
            "                    \n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "\n",
            "                        decoder_output_sequence[target_index] = topi.squeeze().detach() # detach from history as input\n",
            "\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "                else:\n",
            "                    if debugging:\n",
            "                        util.print_mod(f\"NOT USING TEACHER FORCING\", [util.Modifiers.Colors.RED])\n",
            "                    \n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            print(f\"EOS token found at iteration {target_index}\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                \n",
            "                if debugging:\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Target tokenized:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(summary.tolist())\n",
            "\n",
            "                    util.print_mod(\"Target Sequence:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(dataset.detokenize(summary))\n",
            "\n",
            "                    # print tokenized output\n",
            "                    util.print_mod(\"Tokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(decoder_output_sequence.tolist())\n",
            "                    \n",
            "                    util.print_mod(\"Detokenized output:\", [util.Modifiers.Styles.BOLD, util.Modifiers.Styles.ITALIC])\n",
            "                    print(f\"{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            # print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "            util.print_mod(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\", [util.Modifiers.Colors.BLUE, util.Modifiers.Styles.BOLD])\n",
            "\n",
            "train(learning_rate, n_epochs, train_loader, val_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "69323fde9fc4d20886c37f6bdc4a05b4e3b82913212d2329f781a907e0bb44ca"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
