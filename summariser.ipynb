{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\Kevork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from .autonotebook import tqdm as notebook_tqdm\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "NVIDIA GeForce RTX 2080\n"
               ]
            }
         ],
         "source": [
            "# Math and data\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "import polars as pl\n",
            "import math\n",
            "# Neural network frameworks\n",
            "import torch as th\n",
            "from torch import nn\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "from transformers import GPT2Tokenizer, RobertaTokenizer\n",
            "# Utilities\n",
            "import re\n",
            "from enum import Enum\n",
            "import contractions as ct\n",
            "import utility as util\n",
            "import json\n",
            "import random\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "# Plotting\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# Pytorch device\n",
            "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n",
            "if device.type == \"cuda\":\n",
            "    print(th.cuda.get_device_name(device))\n",
            "else:\n",
            "    print(device)\n",
            "\n",
            "#device = th.device(\"cpu\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' def write_dict_to_file(dictionary, file_name):\\n    with open(file_name, \\'w\\') as f:\\n        json.dump(dictionary, f)\\n\\nwrite_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \\n'"
                  ]
               },
               "execution_count": 2,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# instantiate roberta tokenizer\n",
            "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
            "\n",
            "# write dictionary to file\n",
            "\"\"\" def write_dict_to_file(dictionary, file_name):\n",
            "    with open(file_name, 'w') as f:\n",
            "        json.dump(dictionary, f)\n",
            "\n",
            "write_dict_to_file(tokenizer.get_vocab(), \"roberta_vocab.txt\") \n",
            "\"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\n# Load dataset -> Prune dataset -> Tokenize dataset\\ndf = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\\ndf = util.prune(df)\\nutil.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\\ndf = util.tokenize(df, tokenizer)\\n\\n# Find max token length of review text with numpy\\nmax_review_len = np.max(list(df[\\'reviewText\\'].apply(list).apply(len)))\\nprint(\"\\nMax token length of review text: \", max_review_len)\\n# Find max token length of summary with numpy\\nmax_summary_len = np.max((list(df[\\'summary\\'].apply(list).apply(len))))\\nprint(\"Max token length of summary: \", max_summary_len) '"
                  ]
               },
               "execution_count": 3,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Use simple GPT2 tokenizer for counting tokens\n",
            "\"\"\" tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
            "\n",
            "# Load dataset -> Prune dataset -> Tokenize dataset\n",
            "df = util.load_dataset(util.Paths.arts, util.DatasetType.ORIGINAL)\n",
            "df = util.prune(df)\n",
            "util.save_dataset(df, util.Paths.arts, util.DatasetType.PRUNED)\n",
            "df = util.tokenize(df, tokenizer)\n",
            "\n",
            "# Find max token length of review text with numpy\n",
            "max_review_len = np.max(list(df['reviewText'].apply(list).apply(len)))\n",
            "print(\"\\nMax token length of review text: \", max_review_len)\n",
            "# Find max token length of summary with numpy\n",
            "max_summary_len = np.max((list(df['summary'].apply(list).apply(len))))\n",
            "print(\"Max token length of summary: \", max_summary_len) \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# torch dataset from pandas dataframe\n",
            "# defines a voacbulary of words and converts the review text to a list of indices\n",
            "# beware of symbols like ., !, ? etc.\n",
            "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
            "\n",
            "\"\"\"\n",
            "ReviewDataset pytorch dataset interface\n",
            "- expects a polars dataframe with columns reviewText, summary, overall\n",
            "- expects it in the DatasetType.PRUNED format\n",
            "- expects a GPT2Tokenizer\n",
            "\"\"\"\n",
            "class ReviewDataset(Dataset):\n",
            "    def __init__(self, path: str, tokenizer: GPT2Tokenizer, dataset_type = util.DatasetType.PRUNED, device = \"cpu\"):\n",
            "        self.df = util.load_dataset(path, dataset_type)\n",
            "        self.dataset_type = dataset_type\n",
            "\n",
            "        match path:\n",
            "            case util.Paths.arts:\n",
            "                self.max_review_len = util.MaxTokenLength.ARTS_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.ARTS_SUMMARY\n",
            "            case util.Paths.video:\n",
            "                self.max_review_len = util.MaxTokenLength.VIDEO_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.VIDEO_SUMMARY\n",
            "            case util.Paths.gift:\n",
            "                self.max_review_len = util.MaxTokenLength.GIFT_REVIEW\n",
            "                self.max_summary_len = util.MaxTokenLength.GIFT_SUMMARY\n",
            "            case _:\n",
            "                raise ValueError(\"Invalid path\")\n",
            "        \n",
            "        self.tokenizer = tokenizer\n",
            "        self.device = device\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.df)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        review = self.df[\"reviewText\"][idx]\n",
            "        summary = self.df[\"summary\"][idx]\n",
            "        rating = th.tensor(self.df[\"overall\"][idx])\n",
            "\n",
            "        # Tokenize the review and summary strings\n",
            "        review = self.tokenizer.encode(review, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_review_len, return_tensors = \"pt\").squeeze()\n",
            "        summary = self.tokenizer.encode(summary, add_special_tokens = True, padding = \"max_length\", truncation = True, max_length=self.max_summary_len, return_tensors = \"pt\").squeeze()\n",
            "\n",
            "        # move tensors to device\n",
            "        review = review.to(self.device)\n",
            "        summary = summary.to(self.device)\n",
            "        rating = rating.to(self.device)\n",
            "        \n",
            "        return review, summary, rating\n",
            "    \n",
            "    def detokenize(self, x: th.Tensor):\n",
            "        return self.tokenizer.decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
            "\n",
            "    def batch_detokenize(self, x: th.Tensor):\n",
            "        return [self.detokenize(x[i]) for i in range(len(x))]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Review:  what is to go wrong with a gift card. as long as it enters into a person's account as a credit it is just what you paid for.\n",
                  "Summary:  gift card\n",
                  "Rating: 1\n",
                  "MAX_LENGTH: 50258\n"
               ]
            }
         ],
         "source": [
            "# Test the dataset\n",
            "# Setup\n",
            "t = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "t.pad_token = t.eos_token\n",
            "t.add_special_tokens({\"bos_token\": util.BOS_token})\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.gift, t, device=device)\n",
            "\n",
            "data_idx = 45\n",
            "# print(f\"Review: {dataset[data_idx][0]}\")\n",
            "\n",
            "# decode\n",
            "print(f\"Review: {ReviewDataset.detokenize(dataset, dataset[data_idx][0])}\")\n",
            "print(f\"Summary: {ReviewDataset.detokenize(dataset, dataset[data_idx][1])}\")\n",
            "print(f\"Rating: {int(dataset[data_idx][2])}\")\n",
            "\n",
            "# max length is the max index of the vocabulary\n",
            "MAX_LENGTH = len(t)\n",
            "print(f\"MAX_LENGTH: {MAX_LENGTH}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\"\n",
            "Model\n",
            "\"\"\"\n",
            "\n",
            "class EncoderRNN(nn.Module):\n",
            "    def __init__(self, input_size, hidden_size):\n",
            "        super(EncoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "\n",
            "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
            "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
            "\n",
            "    def forward(self, input, hidden):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        output = embedded\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "        return output, hidden\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)\n",
            "\n",
            "class AttnDecoderRNN(nn.Module):\n",
            "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
            "        super(AttnDecoderRNN, self).__init__()\n",
            "        self.hidden_size = hidden_size\n",
            "        self.output_size = output_size\n",
            "        self.dropout_p = dropout_p\n",
            "        self.max_length = max_length\n",
            "\n",
            "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
            "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
            "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
            "        self.dropout = nn.Dropout(self.dropout_p)\n",
            "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
            "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
            "\n",
            "    def forward(self, input, hidden, encoder_outputs):\n",
            "        embedded = self.embedding(input).view(1, 1, -1)\n",
            "        embedded = self.dropout(embedded)\n",
            "\n",
            "        attn_weights = nn.functional.softmax(\n",
            "            self.attn(th.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
            "        attn_applied = th.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
            "\n",
            "        output = th.cat((embedded[0], attn_applied[0]), 1)\n",
            "        output = self.attn_combine(output).unsqueeze(0)\n",
            "\n",
            "        output = nn.functional.relu(output)\n",
            "        output, hidden = self.gru(output, hidden)\n",
            "\n",
            "        output = nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
            "        return output, hidden, attn_weights\n",
            "\n",
            "    def initHidden(self):\n",
            "        return th.zeros(1, 1, self.hidden_size, device=device)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "' hidden_size = 256\\nencoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\\ndecoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\\n\\n# dl = DataLoader(dataset)\\n# dl_it = iter(dl)\\n\\n# Take input from the dataset\\ninput_tensor, target_tensor, rating_tensor = dataset[data_idx]\\n# print(input_tensor.get_device())\\n\\n# Create the encoder hidden state\\nencoder_hidden = encoder.initHidden()\\n\\n# Initialise the encoder output\\nencoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\\n\\n# Run the encoder\\nfor token in input_tensor:\\n    # print(token)\\n    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\\n    encoder_outputs[token] = encoder_output[0, 0]\\n\\nbos = th.tensor(t.bos_token_id).to(device)\\n\\n# Create the decoder input\\ndecoder_input = th.tensor([bos], device=device, dtype=th.long)\\n# Create the decoder output\\ndecoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\\n\\n# Create the decoder hidden state\\ndecoder_hidden = encoder_hidden\\n\\n# Run the decoder\\nfor i, target in enumerate(target_tensor):\\n    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\\n    topv, topi = decoder_output.topk(1)\\n    decoder_input = topi.squeeze().detach() # detach from history as input\\n    \\n    # Append the output\\n    decoder_output_sequence[i] = decoder_input\\n\\n    if decoder_input.item() == t.eos_token_id:\\n        print(f\"EOS token found at {i}th iteration\")\\n        break\\n\\n# Print the output before detokenization\\nprint(f\"Output:\\n{decoder_output_sequence}\\n\")\\n\\n# Print the detokenized output\\nprint(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") '"
                  ]
               },
               "execution_count": 7,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Test the model with a single forward pass\n",
            "\"\"\" hidden_size = 256\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device)\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device)\n",
            "\n",
            "# dl = DataLoader(dataset)\n",
            "# dl_it = iter(dl)\n",
            "\n",
            "# Take input from the dataset\n",
            "input_tensor, target_tensor, rating_tensor = dataset[data_idx]\n",
            "# print(input_tensor.get_device())\n",
            "\n",
            "# Create the encoder hidden state\n",
            "encoder_hidden = encoder.initHidden()\n",
            "\n",
            "# Initialise the encoder output\n",
            "encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "# Run the encoder\n",
            "for token in input_tensor:\n",
            "    # print(token)\n",
            "    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "    encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "# Create the decoder input\n",
            "decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "# Create the decoder output\n",
            "decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.long)\n",
            "\n",
            "# Create the decoder hidden state\n",
            "decoder_hidden = encoder_hidden\n",
            "\n",
            "# Run the decoder\n",
            "for i, target in enumerate(target_tensor):\n",
            "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "    topv, topi = decoder_output.topk(1)\n",
            "    decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "    \n",
            "    # Append the output\n",
            "    decoder_output_sequence[i] = decoder_input\n",
            "\n",
            "    if decoder_input.item() == t.eos_token_id:\n",
            "        print(f\"EOS token found at {i}th iteration\")\n",
            "        break\n",
            "\n",
            "# Print the output before detokenization\n",
            "print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "# Print the detokenized output\n",
            "print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\") \"\"\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Target sequence:\n",
                  " i recommend this because it is so convenient to use.\n",
                  "Detokenized output:\n",
                  " slew slew potent dividingô fateful manager scorevance dividinguserc Nos Nos belts explained increment our explained bloodshed bloodshed low low profession socioeconomic Marriage ArcticQuiteopl helmet Winchester explained Siber explainedgray bloodshed bloodshed lowucci Uber Uber boiled Schn raven WeeksURES equitable07 explained profession Siber dividing dividingrats explained dividing Surf overdose explainedtrained Pinterest026GoodQuiteONEYONEYQuiteONEY\n",
                  "\n",
                  "Epoch: 0, Batch: 0, Loss: 10.859938621520996\n",
                  "Target sequence:\n",
                  " i bought the white and the ecru. the white...\n",
                  "Detokenized output:\n",
                  " explained explained!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 1, Loss: 10.558868408203125\n",
                  "Target sequence:\n",
                  " very nice clasps, magnetic element makes it easy use\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 2, Loss: inf\n",
                  "Target sequence:\n",
                  " but just by viewing them they look great and ready to make some crafts\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 3, Loss: 9.960329055786133\n",
                  "Target sequence:\n",
                  "... as soon as you put a few pencils and fun little crafts in the sections\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 4, Loss: 10.003218650817871\n",
                  "Target sequence:\n",
                  " the replacement blades you need for fiskars rotary cutters 45mm\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 5, Loss: 9.851359367370605\n",
                  "Target sequence:\n",
                  " great tilt for brushes but way too short for brushes\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 6, Loss: inf\n",
                  "Target sequence:\n",
                  " jewelry/earring cards with plastic bags.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 7, Loss: 9.504745483398438\n",
                  "Target sequence:\n",
                  " brass wire 12 gauge...yep that is what i got.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 8, Loss: 9.510111808776855\n",
                  "Target sequence:\n",
                  " they also bend easily. they are too soft to do anything with...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 9, Loss: 9.292414665222168\n",
                  "Target sequence:\n",
                  " this pen does not work like it should\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 10, Loss: inf\n",
                  "Target sequence:\n",
                  " great crucibles for my tabletop furnace!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 11, Loss: inf\n",
                  "Target sequence:\n",
                  " i found these to be completely useless. not only did i experience no benefit at...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 12, Loss: 8.646543502807617\n",
                  "Target sequence:\n",
                  " wish i would have looked longer for a different bead, jewelry kit.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 13, Loss: 8.572528839111328\n",
                  "Target sequence:\n",
                  " false advertising. received only one pair of scissors when...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 14, Loss: inf\n",
                  "Target sequence:\n",
                  " i ordered a 24 color set, i got a...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 15, Loss: 8.349957466125488\n",
                  "Target sequence:\n",
                  " they are smaller than they appear in photos.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 16, Loss: 8.36723518371582\n",
                  "Target sequence:\n",
                  " these are exactly what the price tell you.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 17, Loss: inf\n",
                  "Target sequence:\n",
                  "... and destruction of my new sewing machine -- absolutely wonderful!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 18, Loss: inf\n",
                  "Target sequence:\n",
                  "... thumbprint tree guestbook for my wedding and they worked awesome. i had previously ordered a different brand of...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 19, Loss: 8.091928482055664\n",
                  "Target sequence:\n",
                  " perfect template that does not skid or slip.\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 20, Loss: inf\n",
                  "Target sequence:\n",
                  " perfect; i wanted a medium sized crow skull on...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 21, Loss: 7.860706329345703\n",
                  "Target sequence:\n",
                  " best customer service i have received\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 22, Loss: 8.19257640838623\n",
                  "Target sequence:\n",
                  " but only three stars because they deform and bend pretty easily. and the little threader thing is useless\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 23, Loss: inf\n",
                  "Target sequence:\n",
                  " i wish i had gotten this sewing machine led lighting kit sooner!\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 24, Loss: inf\n",
                  "Target sequence:\n",
                  " great product. i expect nothing less from the warm...\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n",
                  "Epoch: 0, Batch: 25, Loss: 7.917316436767578\n",
                  "Target sequence:\n",
                  " did you know these pads are removable?\n",
                  "Detokenized output:\n",
                  "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
                  "\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[1;32mIn[8], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m             writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLoss/train\u001b[39m\u001b[39m\"\u001b[39m, batch_loss\u001b[39m/\u001b[39mwords_in_batch, epoch \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(train_loader) \u001b[39m+\u001b[39m batch_idx)\n\u001b[0;32m    118\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Batch: \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mbatch_loss\u001b[39m/\u001b[39mwords_in_batch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
                  "Cell \u001b[1;32mIn[8], line 110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDetokenized output:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mdetokenize(decoder_output_sequence)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[39m# Backpropagate the loss\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    112\u001b[0m \u001b[39m# Update the weights\u001b[39;00m\n\u001b[0;32m    113\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mstep()\n",
                  "File \u001b[1;32mc:\\Users\\Kevork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
                  "File \u001b[1;32mc:\\Users\\Kevork\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
                  "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "\"\"\" \n",
            "Training\n",
            "\"\"\"\n",
            "n_epochs = 10\n",
            "batch_size = 1\n",
            "learning_rate = 0.001\n",
            "teacher_forcing_ratio = 0.5\n",
            "#hidden_size = 2**9 # 512\n",
            "hidden_size = 2**10\n",
            "\n",
            "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True, add_prefix_space=True, trim_offsets=True)\n",
            "criterion = nn.CrossEntropyLoss() # TODO: Check without the ignore_index\n",
            "#criterion = nn.NLLLoss()\n",
            "\n",
            "encoder = EncoderRNN(MAX_LENGTH, hidden_size).to(device).train()\n",
            "decoder = AttnDecoderRNN(hidden_size, MAX_LENGTH, dropout_p=0.1).to(device).train()\n",
            "\n",
            "encoder_optimizer = th.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
            "decoder_optimizer = th.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
            "\n",
            "# Create the dataset\n",
            "dataset = ReviewDataset(util.Paths.arts, t, device=device)\n",
            "\n",
            "# Calculate the number of elements in each bucket\n",
            "split_ratios = [0.7, 0.2, 0.1]\n",
            "num_reviwes = len(dataset)\n",
            "n_train_dataset = math.floor(num_reviwes * split_ratios[0])\n",
            "n_validate_dataset = math.floor(num_reviwes * split_ratios[1])\n",
            "n_test_dataset = num_reviwes - n_train_dataset - n_validate_dataset\n",
            "assert n_train_dataset + n_validate_dataset + n_test_dataset == num_reviwes\n",
            "#print(len(dataset))\n",
            "#print(n_train_dataset +n_validate_dataset + n_test_dataset)\n",
            "train_dataset, val_dataset, test_dataset = th.utils.data.random_split(dataset, [n_train_dataset, n_validate_dataset, n_test_dataset])\n",
            "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
            "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
            "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
            "\n",
            "batch_sequence = next(iter(train_loader))\n",
            "\n",
            "# Readying the writer\n",
            "writer = SummaryWriter()\n",
            "\n",
            "# Training loop\n",
            "def train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
            "    for epoch in range(n_epochs):\n",
            "        for batch_idx, (train_review_batch, train_summary_batch, train_rating_batch) in enumerate(train_loader):\n",
            "            batch_loss = 0\n",
            "            words_in_batch = 0\n",
            "\n",
            "            # We calcualte the loss and backpropagate every batch\n",
            "            # Reset the gradients\n",
            "            encoder_optimizer.zero_grad()\n",
            "            decoder_optimizer.zero_grad()\n",
            "\n",
            "            for review, summary, rating in zip(train_review_batch, train_summary_batch, train_rating_batch):\n",
            "                # Create the encoder hidden state\n",
            "                encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "                # Initialise the encoder output's \"feature space\"\n",
            "                encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "                # Run the encoder\n",
            "                for token in review:\n",
            "                    encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                    encoder_outputs[token] = encoder_output[0, 0]\n",
            "                \n",
            "                bos = th.tensor(t.bos_token_id).to(device)\n",
            "\n",
            "                # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "                decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "                # Initialize the decoder output\n",
            "                decoder_output_sequence = th.zeros(dataset.max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "                # Propagate the decoder hidden state\n",
            "                decoder_hidden = encoder_hidden\n",
            "\n",
            "                use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
            "\n",
            "                if use_teacher_forcing:\n",
            "                    # Teacher forcing: Feed the target as the next input\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                        decoder_input = target # Teacher forcing\n",
            "                else:\n",
            "                    # Run the decoder\n",
            "                    for target_index, target in enumerate(summary):\n",
            "                        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                        topv, topi = decoder_output.topk(1)\n",
            "                        decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                        \n",
            "                        # Append the output\n",
            "                        decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                        if decoder_input.item() == t.eos_token_id:\n",
            "                            #print(f\"EOS token found at {target_index}th iteration\")\n",
            "                            # Print the detokenized output\n",
            "                            # Print the target sequence\n",
            "                            # print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                            # print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "                            break\n",
            "\n",
            "                        words_in_batch += 1\n",
            "                        # Calculate the loss\n",
            "                        batch_loss += criterion(decoder_output.squeeze(), target)\n",
            "                print(f\"Target sequence:\\n{dataset.detokenize(summary)}\")\n",
            "                print(f\"Detokenized output:\\n{dataset.detokenize(decoder_output_sequence)}\\n\")\n",
            "            # Backpropagate the loss\n",
            "            batch_loss.backward()\n",
            "\n",
            "            # Update the weights\n",
            "            encoder_optimizer.step()\n",
            "            decoder_optimizer.step()\n",
            "\n",
            "            # Print the loss\n",
            "            writer.add_scalar(\"Loss/train\", batch_loss/words_in_batch, epoch * len(train_loader) + batch_idx)\n",
            "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {batch_loss/words_in_batch}\")\n",
            "                \n",
            "train(learning_rate, n_epochs, train_loader, valid_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\"\"\" # Test the model\n",
            "def test(test_loader, encoder, decoder):\n",
            "    for batch_idx, (test_review_batch, test_summary_batch, test_rating_batch) in enumerate(test_loader):\n",
            "        for review, summary, rating in zip(test_review_batch, test_summary_batch, test_rating_batch):\n",
            "            # Create the encoder hidden state\n",
            "            encoder_hidden = encoder.initHidden() # Can be understood as the context vector\n",
            "\n",
            "            # Initialise the encoder output's \"feature space\"\n",
            "            encoder_outputs = th.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
            "\n",
            "            # Run the encoder\n",
            "            for token in review:\n",
            "                encoder_output, encoder_hidden = encoder(token, encoder_hidden)\n",
            "                encoder_outputs[token] = encoder_output[0, 0]\n",
            "\n",
            "            # Create the decoder input, the beginning of the sequence, starting with the BOS (Beginning Of String) token\n",
            "            decoder_input = th.tensor([bos], device=device, dtype=th.long)\n",
            "\n",
            "            # Create the decoder output\n",
            "            decoder_output_sequence = th.zeros(max_summary_len, device=device, dtype=th.float)\n",
            "\n",
            "            # Propagate the decoder hidden state\n",
            "            decoder_hidden = encoder_hidden\n",
            "\n",
            "            # Run the decoder\n",
            "            for target_index, target in enumerate(summary):\n",
            "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
            "                topv, topi = decoder_output.topk(1)\n",
            "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
            "                \n",
            "                # Append the output\n",
            "                decoder_output_sequence[target_index] = decoder_input\n",
            "\n",
            "                if decoder_input.item() == t.eos_token_id:\n",
            "                    print(f\"EOS token found at {target_index}th iteration\")\n",
            "                    break\n",
            "\n",
            "            # Print the output before detokenization\n",
            "            print(f\"Output:\\n{decoder_output_sequence}\\n\")\n",
            "\n",
            "            # Print the detokenized output\n",
            "            print(f\"Detokenized output:\\n{ReviewDataset.detokenize(dataset, decoder_output_sequence)}\\n\")\n",
            "\n",
            "test(test_loader, encoder, decoder) \"\"\""
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.8"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "3947390f1d33bc3faf6525b63e52b94589ca7e97a4d338993fef3014bbdcaff9"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
