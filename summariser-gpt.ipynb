{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import polars as pl\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# define device\n",
    "device = th.device(\"mps\") if th.backends.mps.is_available() else th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  50257\n",
      "Tokens:  {'input_ids': [64, 1, 64], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input = \"a\\\"a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(\"Vocab size: \", tokenizer.vocab_size)\n",
    "# ouput vocab to file\n",
    "f = open(\"vocab.txt\", \"w\", encoding=\"utf-8\")\n",
    "f.write(str(vocab))\n",
    "\n",
    "# Tokenize input\n",
    "tokens = tokenizer(input)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = GPT2Config()\n",
    "model = GPT2Model(config)\n",
    "\n",
    "input_tensor = th.tensor([tokens['input_ids']])\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "len(output[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          reviewtext  overall  \\\n",
      "0  I had big expectations because I love English ...        2   \n",
      "1  I highly recommend this series. It is a must f...        5   \n",
      "2  This one is a real snoozer. Don't believe anyt...        1   \n",
      "3  Mysteries are interesting.  The tension betwee...        4   \n",
      "4  This show always is excellent, as far as briti...        5   \n",
      "\n",
      "                          summary  \n",
      "0      A little bit boring for me  \n",
      "1           Excellent Grown Up TV  \n",
      "2           Way too boring for me  \n",
      "3     Robson Green is mesmerizing  \n",
      "4  Robson green and great writing  \n",
      "I had big expectations because I love English TV, in particular Investigative and detective stuff but this guy is really boring. It didn't appeal to me at all.\n",
      "28\n",
      "\n",
      "Max length of review text:  18152\n",
      "Max length of summary:  151\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "data_dir = \"datasets/\"\n",
    "data_all = \"All_Amazon_Review_5.json\"\n",
    "data_video = \"Amazon_Instant_Video_5.json\"\n",
    "\n",
    "\n",
    "# read data with pandas\n",
    "df = pd.read_json(data_dir + data_video, lines=True)\n",
    "\n",
    "# lower case all headers\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "# keep only the review text, rating, and summary\n",
    "df = df[['reviewtext', 'overall', 'summary']]\n",
    "print(df.head())\n",
    "\n",
    "# find max length of review text with numpy\n",
    "max_review_len = np.max(df['reviewtext'].apply(len))\n",
    "print(\"\\nMax length of review text: \", max_review_len)\n",
    "# find max length of summary with numpy\n",
    "max_summary_len = np.max(df['summary'].apply(len))\n",
    "print(\"Max length of summary: \", max_summary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prune a dataframe to only contain the columns we need \"\"\"\n",
    "# Drop all rows without a reviewtext or summary\n",
    "df = df.dropna(subset=[\"reviewtext\", \"summary\", \"overall\"])\n",
    "\n",
    "# Write reviewtext, summary or overall to json file\n",
    "df[[\"reviewtext\", \"summary\", \"overall\"]].to_json(\"Pruned_Arts_Crafts_and_Sewing.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_json() got an unexpected keyword argument 'skiprows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(data_dir \u001b[39m+\u001b[39m data_all, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skiprows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, nrows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: read_json() got an unexpected keyword argument 'skiprows'"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(data_dir + data_all, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset from pandas dataframe\n",
    "# defines a voacbulary of words and converts the review text to a list of indices\n",
    "# beware of symbols like ., !, ? etc.\n",
    "# pad the review text and summary to max_review_len and max_summary_len respectively\n",
    "\n",
    "class ReviewDataset(th.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # lazy loading\n",
    "\n",
    "        # move tensors to device\n",
    "        review = review.to(device)\n",
    "        rating = rating.to(device)\n",
    "        summary = summary.to(device)\n",
    "        \n",
    "        return review, rating, summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69323fde9fc4d20886c37f6bdc4a05b4e3b82913212d2329f781a907e0bb44ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
